## **Chapter 21.** Natural Language Processing (NLP) and Large Language Models (LLM)

Recommended Readings **:** 【Algorithms】 [Table of Algorithm Contents](https://jb243.github.io/pages/1278)

---

**1.** [Natural Language Processing](#1-natrual-language-processing-nlp)

**2.** [Large Language Models](#2-large-language-models-llm)

**3.** [Bioinformatics and Language Models](#3-bioinformatics-and-language-models)

---

**a.** [Prompt Engineering](https://jb243.github.io/pages/2317)

**b.** [Natural Language Processing and LLM Useful Function Collection](https://jb243.github.io/pages/2404)

**c.** [Research Topics related to LLM](https://jb243.github.io/pages/194)

---

<br>

## **1\. Natural Language Processing** (NLP)

⑴ Definition **:** AI models based on text

⑵ Text Preprocessing **:** Preprocessing to make unstructured text recognizable to computers

> ① Tokenization

>> ○ Dividing sentences or corpora into minimum meaning units, tokens, for computer recognition

>> ○ **English** **:** Mainly divided by spaces

>>> ○ 75 English words ≃ 100 tokens

>>> ○ Example **:** I / ate / noodle / very / deliciously

>> ○ Example **:** [OpenAI Tokenizer](https://platform.openai.com/tokenizer)

> ② Part-of-Speech Tagging (POS tagging)

>> ○ Technique to tag the parts of speech of morphemes

> ③ Lemmatization

>> ○ Technique to find lemmas (base words) from words

>> ○ Example **:** am, are, is → be

> ④ Stemming

>> ○ Technique to obtain stems by removing prefixes and suffixes from words

> ⑤ Stopword Removal

>> ○ Technique to handle words that contribute little to actual meaning analysis, such as particles and suffixes

⑶ Text Mining 

> ① Topic Modeling

>> ○ One of the statistical models in the fields of machine learning and natural language processing used to discover abstract topics, referred to as 'topics', within a collection of documents.

>> ○ Used to uncover the hidden meaning structure in the text body.

> ② Word Cloud

>> ○ A visualization technique that uses natural language processing to simply count and visualize people's interests or frequency.

> ③ Social Network Analysis (SNA)

>> ○ An analytical technique for analyzing and visualizing the network characteristics and structure among people within a group.

> ④ TF-IDF (Term Frequency-Inverse Document Frequency)

>> ○ A technique used to extract how important a word is within a specific document, in a collection of multiple documents.

⑷ Types

> ① **Type 1:** Latent Semantic Analysis (LSA)

> ② **Type 2:** Probabilistic Latent Semantic Analysis (PLSA)

> ③ **Type 3:** Latent Dirichlet Allocation (LDA): Generative probabilistic models. Can be used even for deconvolution without reference ([ref](https://www.nature.com/articles/s41467-022-30033-z#Sec11))

⑸ Evaluation of Language Models

> ① Parallel text datasets **:** Canadian Parliament (English ← Spanish), European Parliament (multiple languages ​​supported)

> ② SNS

> ③ Bleu score ([ref](https://dl.acm.org/doi/10.3115/1073083.1073135))

> ④ perplexity

<br>

<br>

## **2\. Large Language Models** (LLM)

⑴ Definition **:** Natural language processing models with billions of parameters

> ① Counting parameters method: [here](https://jb243.github.io/pages/2152#:~:text=%E2%97%8B-,Counting%20the%20Number%20of%20Parameters,-%3D%20Goal%20of%20the)

> ② Terms 1: Meaning of 7B, 13B, 30B, 65B **:** The number of parameters in natural language models is 7 billion, 13 billion, 30 billion, and 65 billion

> ③ Term 2: Token **:** The unit of text processed by the model

>> ○ Word-level tokenization **:** [**ChatGPT**, **is**, **an**, **AI**, **language**, **model**, **.**]

>> ○ Subword-level tokenization **:** [**Chat**, **G**, **PT**, **is**, **an**, **AI**, **language**, **model**, **.**]

>> ○ Character-level tokenization **:** [**C**, **h**, **a**, **t**, **G**, **P**, **T**, **i**, **s**, **a**, **n**, **A**, **I**, **l**, **a**, **n**, **g**, **u**, **a**, **g**, **e**, **m**, **o**, **d**, **e**, **l**]

> ④ Term 3: Meaning of 0-shot, 1-shot, etc. **:** The number of example inputs given per task

>> ○ 0-shot prompting

<br>

```python
Q: <Question>?
A:
```

<br>

>> ○ Few-shot prompting

<br>

```python
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A:
```

<br>

⑵ **Transformer**

> ① Training Methods of Transformers

>> ○ Next Word Prediction (NWP): The task of predicting the next word within a given context.

>> ○ Next Sentence Prediction (NSP): Creating sentence pairs A and B from the input data and determining whether B is the next sentence following A.

>>> ○ First Sentence: "The quick brown fox jumps over the lazy dog."

>>> ○ Second Sentence: "The dog is not amused."

>>> ○ Prediction Result: Whether the second sentence is the next sentence following the first sentence (e.g., "No").

>> ○ Masked Language Modeling (MLM): Masking certain words in a sentence and predicting the masked words.

> ② Problem Definition: Next Word Prediction

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/f49df9c3-cdd9-4f68-922c-9dcfc9d72350)

**Figure 1.** Transformer Problem Definition

<br>

> ③ **Step 1.** Segment the sentence into multiple tokens and lift each token onto the embedding space

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/2d18d970-6442-4953-891c-0e3cc46b1832)

**Figure 2.** Token Embedding

<br>

> ④ **Step 2.** Assign attention weights to each word, allowing for next word prediction

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/44a99ab6-5cf9-4c4b-8c2d-a5df87ca69fa)

**Figure 3.** Attention Weights and Next Word Prediction

<br>

> ⑤ **Step 3.** Use attention multilayer perceptron to refine attention weights

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/386b605e-ab47-4e7a-a2fb-9c3bc0d35a3e)

**Figure 4.** Attention Multilayer Perceptron

<br>

> ⑥ **Step 4.** Serially connect attention and multilayer perceptron to continuously generate sentences

>> ○ To date, the cleverest thinker of all time was ___ → undoubtedly

>> ○ To date, the cleverest thinker of all time was undoubtedly ___ → Einstein

>> ○ To date, the cleverest thinker of all time was undoubtedly Einstein, ___ → for

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/fb27f85d-59e2-4a07-857a-d1148794aeb0)

**Figure 5.** Serial Circuit of Attention and Perceptrons

<br>

> ⑦ Hierarchy of Transformers

>> ○ **Step 1:** Initial Layer

>>> ○ At this stage, there is significant variation in the parts of the input features that different attention heads in each layer focus on.

>>> ○ This stage involves broad exploration to distinguish between important and less important information.

>> ○ **Step 2:** Intermediate Layer

>>> ○ At this stage, the attention heads in each layer consistently focus on low-ranking information.

>>> ○ It identifies noise inherent in the data and helps in understanding the finer details.

>> ○ **Step 3:** Final Layer

>>> ○ At this stage, the attention heads in each layer consistently focus on high-ranking information.

>>> ○ This allows for the extraction of decisive information and contributes to making the final decision.

⑶ Important parameters

> ① temperature = 0: leading to very deterministic outputs 

> ② max_tokens API Call settings = 100: response is capped at 100 tokens

> ③ top_p = 1.0: uses all possible tokens for generating the response

> ④ frequency_penalty = 0.0: does not avoide repeating tokens more than it naturally would

> ⑤ presence_penalty = 0.0: no penalty applied for reusing tokens

⑷ Types

> ① [BERT](https://arxiv.org/abs/1810.04805), [RoBERTa](https://arxiv.org/abs/1907.11692), [ALBERT](https://arxiv.org/abs/1909.11942)

>> ○ Using a bidirectional transformer, which is advantageous for understanding text as it considers the context of both the left and right sides of the input sequence.

>> ○ The input for BERT is a maximum of 512 tokens, but the input can be increased by extending the positional embedding: 2<sup>n</sup> is commonly used as the maximum input.

>> ○ A function that loads BERT or BioBERT from Hugging Face to create an attention matrix for a given sentence.

<br>

```
import torch
from transformers import BertTokenizer, BertModel
import matplotlib.pyplot as plt
import seaborn as sns

# Load BERT model and tokenizer (OPTION 1)
'''
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, output_attentions=True)
'''

# Load BioBERT model and tokenizer (OPTION 2)
model_name = 'dmis-lab/biobert-base-cased-v1.1'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name, output_attentions=True)

# Input sentence
sentence = "Find diseases associated with glucose"

# Tokenization
inputs = tokenizer(sentence, return_tensors='pt')

# Calculate outputs and attention weights through the model
outputs = model(**inputs)
attentions = outputs.attentions  # These are the attention weights for each layer

# Visualize the attention weights from the first head of the first layer
attention = attentions[0][0][0].detach().numpy()

# Token list
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

# Visualize Attention weights
plt.figure(figsize=(10, 10))
sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis')
plt.title('Attention Weights')
plt.show()
```

<br>

> ② [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165), GPT-J, GPT-Neo, GPT-3.5, GPT-4

>> ○ Focuses on predicting the next word by sequentially considering the context from left to right in the input sequence.

>> ○ Uses an autoregressive model.

> ③ Comparison between BERT and GPT

<br>

|         | BERT       | GPT-4      |
|---------|------------|------------|
| Developer  | Google AI  | OpenAI     |
| Input Data | Considers both left and right context of input sequences | Considers input sequences sequentially from left to right |
| Parameters | 1.5 B      | 340 M (= 0.34 B) |
| Training Method | NWP        | MLM, NSP   |
| Training Data | 3 TB       | 45 TB      |
| Main Application Area | Text Understanding | Text Generation |

**Table. 1.** Comparison between BERT and GPT

<br>

> ④ Gopher

> ⑤ [Chinchilla](https://arxiv.org/abs/2203.15556)

> ⑥ Flan, PaLM, Flan-PaLM

> ⑦ OPT-IML

> ⑧ LLaMA, **LLaMA2** **:** Installed models. Developed by Meta

> ⑨ Alpaca

> ⑩ [XLNet](https://arxiv.org/abs/1906.08237), [T5](https://arxiv.org/abs/1910.10683), [CTRL](https://arxiv.org/abs/1909.05858), [BART](https://arxiv.org/abs/1910.13461)

> ⑪ [ollama](https://github.com/ollama/ollama) **:** Supports the following LLMs.

>> ○ Llama2 (7B)

>> ○ Mistral (7B)

>> ○ Dolphin Phi (2.7B)

>> ○ Phi-2 (2.7B)

>> ○ Neural Chat (7B)

>> ○ Starling (7B)

>> ○ Code Llama (7B)

>> ○ Llama2 Uncensored (7B)

>> ○ Llama2 (13B)

>> ○ Llama2 (70B)

>> ○ Orca Mini (3B)

>> ○ Vicuna (7B)

>> ○ LLaVA (7B)

> ⑫ [AlphaGeometry](https://jb243.github.io/pages/1213): Implements symbolic deduction to solve geometry problems at the International Mathematical Olympiad (IMO) level.

> ⑬ MiniLM

>> ○ Function that transforms arbitrary variable-length natural language sentences into 384-dimensional vectors by considering their meaning ([ref](https://github.com/portrai-io/CELLama/tree/main))

<br>

<br>

> ⑭ Other useful generative AI proprietary tools

>> ○ [GitHub Copilot](https://copilot.microsoft.com)

>> ○ [Perplexity AI](https://www.perplexity.ai)

>> ○ [Consensus](https://consensus.app)

>> ○ [Scite](https://scite.ai/assistant)

>> ○ SciSpace / typeset.io 

>> ○ Elicit.com 

>> ○ [Claude AI](https://claude.ai/new)

>> ○ Elicit AI

>> ○ Research Rabbit

>> ○ [Gemini](https://gemini.google.com/) 

>> ○ [Mistral AI](https://chat.mistral.ai/chat)

>> ○ Tabnine 

>> ○ CodiumAI

>> ○ AWS Code Whisperer

>> ○ Sourcegraph Cody

>> ○ NotebookLM

<br>

<br>

## **3\. Bioinformatics and Language Models**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/44dcb503-67d5-409d-8382-45a3d1ac0083)

![image](https://github.com/JB243/jb243.github.io/assets/55747737/31c4f27a-63a8-4399-9ed9-c92a837ed67b)

<br>

⑴ BioBERT

⑵ BioNER

⑶ SQuAD

⑷ BioASQ

⑸ [PubMedGPT](https://www.mosaicml.com/blog/introducing-pubmed-gpt) (BioMedLM)

⑹ BioGPT 

⑺ [scBERT](https://www.nature.com/articles/s42256-022-00534-z)

⑻ GPT-Neo

⑼ PubMedBERT

⑽ BioLinkBERT

⑾ DRAGON

⑿ BioMedLM

⒀ [Med-PaLM](https://www.nature.com/articles/s41586-023-06291-2), [Med-PaLM M](https://ai.nejm.org/doi/full/10.1056/AIoa2300138)

⒁ [BioMedGPT](https://arxiv.org/abs/2305.17100)

⒂ [tGPT](https://www.sciencedirect.com/science/article/pii/S2589004223006132) 

⒃ [CellLM](https://arxiv.org/abs/2306.04371) 

⒄ [Geneformer](https://pubmed.ncbi.nlm.nih.gov/37258680/): Based on BERT. Uses a transformer encoder-based architecture. Utilized with a pretraining → finetuning approach. Zero-shot capabilities are practically useless.

⒅ [scGPT](https://github.com/bowang-lab/scGPT): Based on GPT. Uses a transformer decoder-based architecture. Utilized with a pretraining → finetuning approach. The zero-shot performance of the pretraining model is also quite excellent.

⒆ [scFoundation](https://www.biorxiv.org/content/10.1101/2023.05.29.542705v2) 

⒇ [SCimilarity](https://www.biorxiv.org/content/10.1101/2023.07.18.549537v1)

⒇ [CellPLM](https://www.biorxiv.org/content/10.1101/2023.10.03.560734v1.full.pdf) 

<br>

---

_Input**:** 2021-12-11 17:34_

_Modified**:** 2023-05-18 11:36_
