## **Chapter 21.** Natural Language Processing (NLP) and Large Language Models (LLM)

Recommended Readings **:** 【Algorithms】 [Table of Algorithm Contents](https://jb243.github.io/pages/1278)

---

**1.** [Natural Language Processing](#1-natrual-language-processing-nlp)

**2.** [Large Language Models](#2-large-language-models-llm)

**3.** [Bioinformatics and Language Models](#3-bioinformatics-and-language-models)

---

**a.** [Prompt Engineering](https://jb243.github.io/pages/2317)

**b.** [Natural Language Processing and LLM Useful Function Collection](https://jb243.github.io/pages/2404)

**c.** [Research Topics related to LLM](https://jb243.github.io/pages/194)

---

<br>

## **1\. Natural Language Processing** (NLP)

⑴ Definition **:** AI models based on text

⑵ Text Preprocessing **:** Preprocessing to make unstructured text recognizable to computers

> ① Tokenization

>> ○ Dividing sentences or corpora into minimum meaning units, tokens, for computer recognition

>> ○ **English** **:** Mainly divided by spaces

>>> ○ 75 English words ≃ 100 tokens

>>> ○ Example **:** I / ate / noodle / very / deliciously

>> ○ Example **:** [OpenAI Tokenizer](https://platform.openai.com/tokenizer)

> ② Part-of-Speech Tagging (POS tagging)

>> ○ Technique to tag the parts of speech of morphemes

> ③ Lemmatization

>> ○ Technique to find lemmas (base words) from words

>> ○ Example **:** am, are, is → be

> ④ Stemming

>> ○ Technique to obtain stems by removing prefixes and suffixes from words

> ⑤ Stopword Removal

>> ○ Technique to handle words that contribute little to actual meaning analysis, such as particles and suffixes

⑶ Text Mining 

> ① Topic Modeling

>> ○ One of the statistical models in the fields of machine learning and natural language processing used to discover abstract topics, referred to as 'topics', within a collection of documents.

>> ○ Used to uncover the hidden meaning structure in the text body.

> ② Word Cloud

>> ○ A visualization technique that uses natural language processing to simply count and visualize people's interests or frequency.

> ③ Social Network Analysis (SNA)

>> ○ An analytical technique for analyzing and visualizing the network characteristics and structure among people within a group.

> ④ TF-IDF (Term Frequency-Inverse Document Frequency)

>> ○ A technique used to extract how important a word is within a specific document, in a collection of multiple documents.

⑷ Transformer

> ① Problem Definition: Next Word Prediction

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/bde7ddfe-c36b-4606-81b9-7a4ce31ad28f)

**Figure 1.** Transformer Problem Definition

<br>

> ② **Step 1.** Segment the sentence into multiple tokens and lift each token onto the embedding space

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/e2c57025-1815-49c8-b8a4-3b73b08d7e2f)

**Figure 2.** Token Embedding

<br>

> ③ **Step 2.** Assign attention weights to each word, allowing for next word prediction

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/ab0dddc7-822f-4317-bd68-9058c6de8328)

**Figure 3.** Attention Weights and Next Word Prediction

<br>

> ④ **Step 3.** Use attention multilayer perceptron to refine attention weights

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/f96cf9ef-f6e7-463e-800b-0848b4d3ba82)

**Figure 4.** Attention Multilayer Perceptron

<br>

> ⑤ **Step 4.** Serially connect attention and multilayer perceptron to continuously generate sentences

>> ○ To date, the cleverest thinker of all time was ___ → undoubtedly

>> ○ To date, the cleverest thinker of all time was undoubtedly ___ → Einstein

>> ○ To date, the cleverest thinker of all time was undoubtedly Einstein, ___ → for

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/c2545473-efd0-43a8-bd4f-1320d2627934)

**Figure 5.** Serial Circuit of Attention and Perceptrons

<br>

⑸ Types

> ① **Type 1:** Latent Semantic Analysis (LSA)

> ② **Type 2:** Probabilistic Latent Semantic Analysis (PLSA)

> ③ **Type 3:** Latent Dirichlet Allocation (LDA)

>> ○ Generative probabilistic models

>> ○ Can be used even for deconvolution without reference ([ref](https://www.nature.com/articles/s41467-022-30033-z#Sec11))

⑹ Evaluation of Language Models

> ① Parallel text datasets **:** Canadian Parliament (English ← Spanish), European Parliament (multiple languages ​​supported)

> ② SNS

> ③ Bleu score ([ref](https://dl.acm.org/doi/10.3115/1073083.1073135))

> ④ perplexity

<br>

<br>

## **2\. Large Language Models** (LLM)

⑴ Definition **:** Natural language processing models with billions of parameters

> ① Counting parameters method:
[here](https://jb243.github.io/pages/2152#:~:text=%E2%97%8B-,Counting%20the%20Number%20of%20Parameters,-%3D%20Goal%20of%20the)

> ② Terms 1: Meaning of 7B, 13B, 30B, 65B **:** The number of parameters in natural language models is 7 billion, 13 billion, 30 billion, and 65 billion

> ③ Term 2: Token **:** The unit of text processed by the model

>> ○ Word-level tokenization **:** [**ChatGPT**, **is**, **an**, **AI**, **language**, **model**, **.**]

>> ○ Subword-level tokenization **:** [**Chat**, **G**, **PT**, **is**, **an**, **AI**, **language**, **model**, **.**]

>> ○ Character-level tokenization **:** [**C**, **h**, **a**, **t**, **G**, **P**, **T**, **i**, **s**, **a**, **n**, **A**, **I**, **l**, **a**, **n**, **g**, **u**, **a**, **g**, **e**, **m**, **o**, **d**, **e**, **l**]

> ④ Term 3: Meaning of 0-shot, 1-shot, etc. **:** The number of example inputs given per task

>> ○ 0-shot prompting

<br>

```python
Q: <Question>?
A:
```

<br>

>> ○ Few-shot prompting

<br>

```python
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A: <Answer>
Q: <Question>?
A:
```

<br>

⑵ Types

> ① [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165), GPT-J, GPT-Neo, GPT-3.5, GPT-4

> ② Gopher

> ③ [Chinchilla](https://arxiv.org/abs/2203.15556)

> ④ Flan, PaLM, Flan-PaLM

> ⑤ OPT-IML

> ⑥ LLaMA, **LLaMA2** **:** Installed models. Developed by Meta

> ⑦ Alpaca

> ⑧ [BERT](https://arxiv.org/abs/1810.04805), [RoBERTa](https://arxiv.org/abs/1907.11692), [ALBERT](https://arxiv.org/abs/1909.11942)

> ⑨ [XLNet](https://arxiv.org/abs/1906.08237), [T5](https://arxiv.org/abs/1910.10683), [CTRL](https://arxiv.org/abs/1909.05858), [BART](https://arxiv.org/abs/1910.13461)

> ⑩ [ollama](https://github.com/ollama/ollama) **:** Supports the following LLMs.

>> ○ Llama2 (7B)

>> ○ Mistral (7B)

>> ○ Dolphin Phi (2.7B)

>> ○ Phi-2 (2.7B)

>> ○ Neural Chat (7B)

>> ○ Starling (7B)

>> ○ Code Llama (7B)

>> ○ Llama2 Uncensored (7B)

>> ○ Llama2 (13B)

>> ○ Llama2 (70B)

>> ○ Orca Mini (3B)

>> ○ Vicuna (7B)

>> ○ LLaVA (7B)

> ⑪ [AlphaGeometry](https://jb243.github.io/pages/1213): Implements symbolic deduction to solve geometry problems at the International Mathematical Olympiad (IMO) level.

> ⑫ Other useful generative AI proprietary tools

>> ○ GitHub Copilot

>> ○ <https://www.perplexity.ai>

>> ○ <https://consensus.app>

>> ○ <https://scite.ai/assistant>

>> ○ SciSpace / typeset.io 

>> ○ Elicit.com 

⑶ Important parameters

> ① temperature = 0: leading to very deterministic outputs 

> ② max_tokens API Call settings = 100: response is capped at 100 tokens

> ③ top_p = 1.0: uses all possible tokens for generating the response

> ④ frequency_penalty = 0.0: does not avoide repeating tokens more than it naturally would

> ⑤ presence_penalty = 0.0: no penalty applied for reusing tokens

⑷ Role of Programmers

> ① **Role 1:** Development of LLM models **:** Not very practical

>> ○ LLM model development requires substantial server resources, making it difficult without the scale of OpenAI, Meta, Google, etc.

>> ○ Even Naver, Kakao have not achieved that scale

>> ○ Some predict that ChatGPT 4.0 might be the final version of LLM unless OpenAI, Meta, Google, etc. join forces

> ② **Role 2:** Fine-tuning

>> ○ Model is already defined, improvement in a specific field by providing significant training data

>> ○ Example **:** [ChatGPT fine-tuning](https://wikidocs.net/200282)

> ③ **Role 3:** Crawling from DB and tagging with LLM

> ④ **Role 4:** [Prompt Engineering](https://jb243.github.io/pages/2317)

> ⑤ **Role 5:** Utilizing existing models with different frontends, backends for various services

⑸ Limitations of ChatGPT

> ① It cannot process images

> ② Lack of timeliness: cannot be solved by parameter tuning

> ③ Lack of creativity: only strives for the average

> ④ Does not understand the real world: the difference between the real world governed by quantum mechanics and the cyber space governed by semiconductors

<br>

<br>

## **3\. Bioinformatics and Language Models**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/99de598e-ca93-4981-b34f-bf390f0aa078)

![image](https://github.com/JB243/jb243.github.io/assets/55747737/ac4efca9-02eb-46b1-87c2-3736e9158175)

<br>

⑴ BioBERT

⑵ BioNER

⑶ SQuAD

⑷ BioASQ

⑸ [PubMedGPT](https://www.mosaicml.com/blog/introducing-pubmed-gpt) (BioMedLM)

⑹ BioGPT 

⑺ [scBERT](https://www.nature.com/articles/s42256-022-00534-z)

⑻ GPT-Neo

⑼ PubMedBERT

⑽ BioLinkBERT

⑾ DRAGON

⑿ BioMedLM

⒀ [Med-PaLM](https://www.nature.com/articles/s41586-023-06291-2), [Med-PaLM M](https://ai.nejm.org/doi/full/10.1056/AIoa2300138)

⒁ [BioMedGPT](https://arxiv.org/abs/2305.17100)

⒂ [tGPT](https://www.sciencedirect.com/science/article/pii/S2589004223006132) 

⒃ [CellLM](https://arxiv.org/abs/2306.04371) 

⒄ [Geneformer](https://pubmed.ncbi.nlm.nih.gov/37258680/) 

⒅ [scGPT](https://github.com/bowang-lab/scGPT) 

⒆ [scFoundation](https://www.biorxiv.org/content/10.1101/2023.05.29.542705v2) 

⒇ [SCimilarity](https://www.biorxiv.org/content/10.1101/2023.07.18.549537v1)

⒇ [CellPLM](https://www.biorxiv.org/content/10.1101/2023.10.03.560734v1.full.pdf) 

<br>

---

_Input**:** 2021-12-11 17:34_

_Modified**:** 2023-05-18 11:36_
