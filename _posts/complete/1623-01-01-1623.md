## **Chapter 3. Probability Space**

Higher category **:** 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [set theory](#1-set-theory)  

**2.** [conditional probability](#2-conditional-probability) 

---

**a.** [Inclusion-Exclusion Principle](https://jb243.github.io/pages/1640) 

**b.** [Monty Hall Problem](https://jb243.github.io/pages/1649) 

**c.** [Difficult statistics problems](https://jb243.github.io/pages/1651) 

---

<br>

## **1. Set theory** 

⑴ case(outcome)**:** can be defined arbitrarily. marked as ω<sub>i</sub>

⑵ sample space**:** set Ω that satisfies the following conditions

> ① Ω = { ω<sub>1</sub>, ω<sub>2</sub>, ···, ω<sub>n</sub> }

> ② ω<sub>i</sub> and ω<sub>j</sub> are disjoint

> ③ Ω includes all possible results

> ④ ((1, 2, 3), (3, 4, 5, 6)) (×)

> ⑤ ((1, 2), (3, 4, 5, 6)) (○)

⑶ field**:** a set F of all subsets of Ω

> ① assumption**:** Ω = {1, 2, 3} 

> ② F = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}

⑷ events **:** each element of F, _i.e._ **subset** of sample space 

> ① total event **:** Ω 

> ② null event **:** ∅

> ③ complementary event **:** A<sup>c</sup> = Ω - A 

> ④ union event**:** A ∪ B

> ⑤ intersection event **:** A ∩ B

> ⑥ mutually exclusive events **:** relationship between events A and B that satisfy A ∩ B = ∅

> ⑦ when number of cases is n, the number of events is 2n 

⑸ probability

> ① intuitive definition**:** probability of a particular event = number of a particular event ÷ number of all cases

> ② axiomatic approach**:** proposed by Kolmogorov, the father of statistics

>> ○ probability is a kind of function **:** F → ℝ

>> ○ **axiom 1**. P(E) ≥ 0

>> ○ **axiom 2**. P(Ω) = 1

>> ○ **axiom 3.** addition rule of probability**:** about disjoint E<sub>i</sub> and E<sub>j</sub>

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/2c799ea8-396b-450f-bb80-0b9cb8c4a3f4" alt="drawing">
</center>
  <br>

>> ○ **theorem 1.** P(∅) = 0 

>>> ○ P(U) = P(U) + P(∅) ⇔ P(∅) = 0

>>> ○ note that even though E ≠ ∅, P(E) can be 0

>> ○ **theorem** **2.** P(E<sup>c</sup>) = 1 - P(E) 

>>> ○ 1 = P(E ∪ E<sup>c</sup>) = P(E) + P(E<sup>c</sup>)

>> ○ **theorem** **3.** E ⊂ F → P(E) ≤ P(F) 

>>> ○ F = E ∪ (E<sup>c</sup> ∩ F) → P(F) = P(E) + P(E<sup>c</sup> ∩ F) ≥ P(E)

>> ○ **theorem** **4.** P(E ∪ F) = P(E) + P(F) - P(E ∩ F)

>>> ○ E ∪ F = (E<sup>c</sup> ∩ F) ∪ (E ∩ F) ∪ (E ∩ F<sup>c</sup>)

>>> ○ P(E ∪ F) = (P(F) - P(E ∩ F)) + P(E ∩ F) + (P(E) - P(E ∩ F)) 

> ③ Cromwell's rule**:** it states that we should not use probabilities of 1 or 0, except for logical statements 

⑹ [inclusion-exclusion principle](https://jb243.github.io/pages/1640) 

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/732a8492-5b17-4ac4-901b-84b46c985fba" alt="drawing">
</center>
  <br>

> ① example **:** P(A ∪ B ∪ C) = P(A) + P(B) + P(C) - P(A ∩ B) - P(B ∩ C) - P(C ∩ A) + P(A ∩ B ∩ C)  

<br>

<br>

## **2. Conditional** **probability**  

⑴ conditional probability

> ① <span>P(A | B)**:** the probability that event A will occur when event B already occurs</span>

> ② <span>expression <b>:</b> P(A | B) = P(A ∩ B) ÷ P(B)</span>

> ③ <span>P(A | B) ≠ P(B | A) </span>

> ④ <span>multiplication rule of probability**:** P(A ∩ B) = P(A) × P(B | A) = P(B) × P(A | B)</span>

> ⑤ Multiplication theorem of probabilities with more than one premise

>> ○ The following equation is self-evident because there are implicitly certain prerequisites (e.g., α, β) for any probability

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/86c71845-2278-4932-9001-ea6e38bdfbcd" alt="drawing">
</center>
  <br>

⑵ chain rule of conditional probability

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/b1e7ad1f-2a64-4bd7-84f5-aa3e694b8aeb" alt="drawing">
</center>
  <br>

> ① <span>example <b>:</b> P(A ∩ B ∩ C) = P(A) × P(B | A) × P(C | A ∩ B)  </span>

⑶ independence of events**:** if the conditional probability is equal to the intrinsic probability

> ① expression**:** P(A ∩ B) = P(A) × P(B)

> ② application

>> ○ <span>P(A | B) = P(A ∩ B) ÷ P(B) = P(A)P(A B) = P(A ∩ B) ÷ P(B) = P(A)</span>

>> ○ <span>P(B | A) = P(A ∩ B) ÷ P(A) = P(B)</span>

>> ○ E and F are independent ⇔ E<sup>c</sup> and F are independent ⇔ E and F<sup>c</sup> are independent ⇔ E<sup>c</sup> and F<sup>c</sup> are independent

>> ○ if P(A) = 0, A and B are always independent

>> ○ P(A), P(B) > 0, and A and B are independent → A and B are not disjoint

>> ○ if A and B are disjoint, P(A) = 0, P(B) = 0, or A and B are not independent (contrapositive proposition)

> ③ intuitive meaning

>> ○ information about one variable does not tell you any information about the other

> ④ relationship with conditional probabilities

>> ○ <span>Pr(X = x | Y = y) = Pr(X = x)</span>

⑷ mutual independence

> ① **condition 1.** 

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/da086de2-a145-4e87-80ef-a5b1a26e8cdd" alt="drawing">
</center>
  <br>

> ② **condition 2.** includes **condition 1**  

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/e349132d-c42c-4ca2-a292-80bd59c4bca3" alt="drawing">
</center>
  <br>

⑸ conditional independence

> ① definition

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/ba8e6440-92dd-4d83-9ca1-f1cd719b8403" alt="drawing">
</center>
  <br>

> ② conditional independence is not always independent

> ③ independence is not always conditional independence

⑹ law of total probability

> ① partition

>> ○ **definition 1.** (exhaustive) Ω = B<sub>1</sub> ∪ B<sub>2</sub> ∪ ··· ∪ B<sub>n</sub>

>> ○ **definition 2.** (disjoint) B<sub>i</sub> ∩ B<sub>j</sub> = ∅

> ② conclusion

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/2e14ba78-9458-4390-b120-9354aa5e6988" alt="drawing">
</center>
  <br>

>> ○ tip**:** drawing a Ben diagram is easy to understand

> ③ **important****:** used to induce Bayes' theorem

⑺ Bayes' theroem

> ① **a****ssumption 1.** S = B<sub>1</sub> ∪ B<sub>2</sub> ∪ ··· ∪ B<sub>n</sub>

> ② **assumption 2.** ∀ i, j, B<sub>i</sub> ∩ B<sub>j</sub> = ∅

> ③ conclusion

  <br>
  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/1d55cf57-b147-4aa3-bfd4-2c9121d3b8be" alt="drawing">
</center>
  <br>

> ④ <span><b>note 1.</b> a totally unrelated P (A | B) may be used to calculate P (B | A)</span>

> ⑤ **note 2.** results (posterior, a posterior) cand predict causes (prior, a pryori)

>> ○ A: events observed as a result 

>> ○ B**:** a causal event

>> ○ P(B<sub>i</sub>) **:** **prior**

>> ○ <span>P(B<sub>i</sub> | A) <b>:</b> when A is observed, the probability that B<sub>i</sub> is the cause (<b>posterior</b>) </span>

>> ○ <span>P(A | B<sub>i</sub>) <b>:</b> when B<sub>i</sub> is the cause, the probability of A occurring (<b>likelihood</b>)</span>

> ⑥ [Bayes Statistic](https://jb243.github.io/pages/1192)

>> ○ infers probabilities without identifying the population

>> ○ claims that probability is nothing but human belief

⑻ **example** **1.** diagnostic rate

> ① percentage of cancer patients

<br>

<center>P(A) = 1%</center>

<br>

> ② positive probability (hit rate) at diagnosis

<br>

<center>P(B)</center>

<br>

> ③ positive probability when diagnosing cancer patients

<br>

<center>P(B | A) = 95%</center>

<br>

> ④ probability of false positive when diagnosing non-cancer patients

<br>

<center>P(B | A<sup>c</sup>) = 5%</center>

<br>

> ⑤ **question :** probability of a positive person really getting cancer during diagnosis

<br>

<center>P(A | B)</center>

<br>

> ⑥ proportion of positive in diagnosis**:** the law of total probability is used

<br>

<center>P(B) = P(B | A) + P(B | A<sup>c</sup>) = 0.95 × 0.01 + 0.05 × 0.99</center>

<br>

> ⑦ probability that any person is a cancer patient and is positive in diagnosis

<br>

<center>P(A ∩ B) = P(B | A) × P(A) = 0.95 × 0.01</center>

<br>

> ⑧ answer

<br>

<center>P(A | B) = P(A ∩ B) ÷ P(B) = **0.16**</center>

<br>

> ⑨ **interpretation :** since there are many people who are not cancer patients, the false positive rate is quite high

⑼ **example 2.** Monty hall problem

> ① situation

>> ○ supercar stands behind one of the three doors

>> ○ show participant choose one of the three doors 

>> ○ the show host opens any of the doors that the show participant did not open

>> ○ show participant can stick to or change their existing selections

>> ○ question**:** which choice is reasonable?

> ② premise

>> ○ show participant first select door 1

>> ○ show host opens door 3

> ③ definition

>> ○ P(i): probability of supercar behind door 1

>> ○ P(ii): probability of supercar behind door 2

>> ○ P(iii): probability of supercar behind door 3

>> ○ P(Ⅰ): probability of the show host choosing door 1

>> ○ P(Ⅱ): probability of the show host choosing door 2

>> ○ P(Ⅲ)**:** probability of the show host choosing door 3

>> ○ P(i), P(ii), and P(iii) are priors (causes)

>> ○ P(Ⅰ), P(Ⅱ), and P(Ⅲ) are posteriors (results)

> ④ conditional probability

>> ○ <span>P(ⅰ | Ⅲ)</span> **:** when the show host selects door 3, the probability that there is a supercar behind door 1

>> ○ <span>P(Ⅲ | ⅰ)</span> **:** if there's a supercar behind door 1, the probability that the show host will choose door 3

> ⑤ Bayes' theorem

>> ○ <span>P(ⅰ | Ⅲ)</span>

<br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/b1c0e4f4-82b2-4199-beba-4840254893b1" alt="drawing">
</center>
  <br>

>> ○ <span>P(ⅱ | Ⅲ)</span>

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/91fc57f4-ea87-4b53-9a43-4f7a270605bd" alt="drawing">
</center>
  <br>

>> ○ <span>P(ⅲ | Ⅲ)</span>

  <br>
<center>
<img src="https://github.com/JB243/jb243.github.io/assets/55747737/99fe301a-e874-4c2c-bc60-4c64c25e3641" alt="drawing">
</center>
  <br>

> ⑥ conclusion**:** it's advantageous for show participant to change their choices

<br>

---

*Input : 2019.06.16 22:02*
