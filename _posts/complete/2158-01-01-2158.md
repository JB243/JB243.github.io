## **Chapter 7. Dimension Reduction Algorithm**

Recommended post **:** 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2.** **Type 1.** [Principal Component Analysis (PCA)](#2-type-1-principal-component-analysis-pca)

**3\. Type 2.** [SNE, symmetric-SNE, tSNE](#3-type-2-sne-symmetric--sne-tsne)

**4\. Type 3.** [UMAP](#4-type-3-umap)

**5\. Type 4.** [SVD](#5-type-4-svd)

**6\. Type 5.** [ICA](#6-type-5-ica)

**7\.** [Others](#7-others)

---

**a.** [SNE, symmetric-SNE, tSNE](https://jb243.github.io/pages/1730)

**b.** [UMAP](https://jb243.github.io/pages/2203) (uniform manifold approximation and projection)

---

<br>

## **1\. Overview**

 ⑴ Definition of Dimension Reduction

> ① When given high-dimensional data X<sub>(1)</sub>, ···, X<sub>(n)</sub> ∈ ℝ<sup>p</sup>,

> ② Find an appropriate [subspace](https://jb243.github.io/pages/1897) V ⊂ ℝ<sup>p</sup> (where dim(V) = k),

> ③ Calculate the projection of X onto V, denoted as X̃.

> ④ It is an unsupervised algorithm.

⑵ Characteristics

> ① Information preservation

> ② Easier model training **:** In machine learning, it requires fewer parameters, so it is faster with lower computation.

> ③ Easier result interpretation **:** Visualization of high-dimensional data.

> ④ Noise reduction

> ⑤ It can reveal the true dimensionality of the given data.

 ⑶ Methods

> ① Feature selection

>> ○ Selecting a few important variables from the existing ones and discarding the rest.

>> ○ Choose one variable with a high correlation or high variance inflation factor (VIF).

> ② Feature extraction

>> ○ Combine all variables to derive new variables that best represent the data.

>> ○ Techniques that combine existing variables to create new ones.

<br>

<br>

## **2\. Type 1.** Principal Component Analysis (PCA)

⑴ Overview

> ① Definition **:** A dimension reduction technique that transforms correlated variables into orthogonal variables.

> ② In other words, it is a dimension reduction method that leaves only a few meaningful coordinate systems.

> ③ The orthogonal variables are called principal components, which can be used when they contain as much information as the original data.

> ④ It originates from the awareness that the given dimensionality and the true dimensionality of the data may differ.

> ⑤ Introduced by Pearson in 1901.

> ⑥ PCA can be defined not only in subspace (passing through the origin) but also in affine space.

> ⑦ **** Normalize by the mean and then perform PCA on the subspace.

> ⑧ PCA does not require multivariate normality of the given data.

⑵ Premises

> ① Represent the basis of the space where the data belongs as e<sub>1</sub>, ···, e<sub>p</sub> (the set of bases does not need to be orthogonal).

>> ○ e<sub>1</sub>, ···, e<sub>p</sub> can be considered as bases representing the given dimensions (no need for orthogonality).

> ② Goal **:** To find the orthogonal basis Z<sub>1</sub>, ···, Z<sub>k</sub> of such a subspace (k < n) (typical unsupervised problem setting).

>> ○ Z<sub>1</sub>, ···, Z<sub>k</sub> can be considered as bases representing the true dimensionality of the data.

<br>

![image](https://github.com/user-attachments/assets/a040256a-0f86-4033-a62e-5a0b740e92d4)

<br>

> ③ **Random vector**

>> ○ X represents a specific data point **:** x<sub>1</sub>, ···, x<sub>n</sub> are each component.

<br>

![image](https://github.com/user-attachments/assets/15c6f2f7-84d2-4c1e-980d-6b64528b5949)

<br>

> ④ **Mean vector** **:** Also called population mean.

<br>

![image](https://github.com/user-attachments/assets/a2a5e5c0-320a-4f1b-a3c0-c2f2d07055ef)

<br>

> ⑤ **Variance-covariance matrix**

<br>

![image](https://github.com/user-attachments/assets/0e9b988b-f0ef-4a41-b8df-e10caf819d54)

<br>

> ⑥ **Unbiased sample variance-covariance matrix** **:** With the design matrix **X**, and center matrix **X<sub>c</sub>**,

<br>

![image](https://github.com/user-attachments/assets/c937618b-0adc-486c-b16d-4eb67d43f50e)

<br>

>> ○ Used when the population variance-covariance matrix is unknown.

>> ○ Since **X** ∈ ℝ<sup>n×p</sup>, **X**<sup>T</sup>**X** ∈ ℝ<sup>p×n</sup> × ℝ<sup>n×p</sup> = ℝ<sup>p×p</sup>.

> ⑦ Expression of the goal orthogonal basis

<br>

![image](https://github.com/user-attachments/assets/08803bfd-bdfb-43bc-8f23-bed4bbf0e675)

<br>

>> ○ Z<sub>k</sub> **:** = k-th PC ∈ ℝ<sup>p×1</sup>.

> ⑧ Characteristics of the goal orthogonal basis **:** Easily derived from linear algebra concepts.

<br>

![image](https://github.com/user-attachments/assets/37c61a4f-5048-4d0b-a22d-073835dcd87a)

<br>

>> ○ Var, Cov, etc., are specific values related to variance, belonging to ℝ<sup>1x1</sup>.

>> ○ Do not confuse with the variance-covariance matrix.

⑶ Definition of the PCA problem

<br>

![image](https://github.com/user-attachments/assets/724c5796-2854-43e5-833b-158fed21bd29)

<br>

⑷ Solution to the PCA problem

> ① **Method 1.** Eigen-decomposition of ∑.

>> ○ Align well along each coordinate axis so that the covariance matrix becomes diagonal.

<br>

![image](https://github.com/user-attachments/assets/7145a9a7-b0c8-45eb-a9a6-80a719e86797)

<br>

>> ○ If we set the directions of data variation along the x, y, z axes, the covariance in the x-y plane becomes 0.

<br>

![image](https://github.com/user-attachments/assets/88dec69c-f837-4878-b0ac-84615d4f2e28)

<br>

>> ○ Note that (**P**<sup>T</sup>**X**)<sup>T</sup> = **X**<sup>T</sup>**P**<sup>TT</sup> = **X**<sup>T</sup>**P**, and E[**P**] = **P**, E[**P**<sup>T</sup>] = **P**<sup>T</sup>.

>> ○ Since **P** is a rotation transformation, the diagonal matrix is also the inverse matrix, leading to the following:

<br>

![image](https://github.com/user-attachments/assets/dba1cb8d-4143-45de-a1ac-39dc2145b70e)

<br>

>> ○ **P** = [ **p<sub>1</sub>**, **p<sub>2</sub>**, ···, **p<sub>N</sub>**], **Z** = cov( **X** ) is defined, so,

<br>

![image](https://github.com/user-attachments/assets/f113dc23-7142-42f4-92fb-2757b41f75dd)

<br>

>> ○ Hence, λ<sub>i</sub> **p<sub>i</sub>** = **Zp<sub>i</sub>**, where λ is the eigenvalue and **p<sub>i</sub>** is the eigenvector.

>> ○ Remove eigenvalues smaller than a certain threshold η, and perform dimension reduction by projecting the data set onto the remaining eigenvectors.

>> ○ **Conclusion 1.** The d-th principal component PC<sub>d</sub> is the eigenvector corresponding to the d-th largest eigenvalue.

>> ○ **Conclusion 2.** The meaning of eigenvectors with the same eigenvalue **:** The magnitude of the corresponding principal component is the same.

> ② **Method 2.** Singular value decomposition (SVD) of the center matrix **X**<sub>c</sub>: The standard method currently used in PCA algorithms.

> ③ **Method 3.** [Lagrange multiplier under equality constraint](https://jb243.github.io/pages/1813).

 ⑸ Determining the number of axes

> ① **Method 1.** Scree plot for the fraction of total variance.

>> ○ Definition of the fraction of total variance.

<br>

![image](https://github.com/user-attachments/assets/13b00d84-8318-4d3c-955c-c3ac61886aab)

<br>

>> ○ Graphical representation.

<br>

![image](https://github.com/user-attachments/assets/da97e824-00d1-4e02-a058-4d54b40e2d7f)

**Figure 1.** Example of axis setting.

<br>

![image](https://github.com/user-attachments/assets/888e17f2-c14b-4b9f-964d-97e2001fdb2c)

**Figure 2.** Scree plot **:** Shows the amount of variance explained by each principal component.

<br>

> ② **Method 2.** Scree plot for eigenvalues.

> ③ **Method 3.** Use in-sample 10-fold cross-validation to determine the number of axes p that minimizes MSPE.

<br>

![image](https://github.com/user-attachments/assets/d9cf92b1-9211-45bd-a984-c8d3c55a966b)

**Figure 3.** Determination of the number of axes.

<br>

> ④ Since information about the remaining axes is lost, PCA functions as a lossy compression algorithm.

 ⑹ Summary

> ① Comparison of prediction performance.

<br>

![image](https://github.com/user-attachments/assets/0cfc430b-7523-415c-9335-ceb85d10317c)

**Figure 4.** Prediction performance comparison.

<br>

## 3\. Type 2.[SNE, symmetric-SNE, tSNE](https://jb243.github.io/pages/1730)

⑴ A dimension reduction algorithm that attempts to preserve local neighborhoods in the data.

⑵ Nonlinear and non-deterministic.

⑶ Requires knowledge of cost function, updating algorithm, etc., related to [deep learning](https://jb243.github.io/pages/1138).

<br>

<br>
 
## 4\. Type 3. [UMAP](https://jb243.github.io/pages/2203)

<br>

<br>

## **5\. Type 4.** Singular Value Decomposition (SVD)

⑴ A method that extracts singular values from an M × N dimensional matrix and reduces the dimensionality of the given data.

⑵ Formula **:** Let A be an m × n matrix with rank r.

> ① Then there exists an m × n matrix ∑ whose diagonal entries are the r nonzero singular values of A, σ<sub>1</sub> ≥ σ<sub>2</sub> ≥ ··· ≥ σ<sub>r</sub> > 0

<br>

<img width="393" alt="스크린샷 2024-10-22 오전 11 44 56" src="https://github.com/user-attachments/assets/3f9d04a4-797c-49ec-9152-4c62dfa74cae">

<br>

> ② and there exists an m × m orthogonal matrix U and an n × n orthogonal matrix V such that

>> ○ The SVD is not unique since we can switch the sign of a column of U and the sign of the corresponding column of V.

<br>

<img width="97" alt="스크린샷 2024-10-22 오전 11 34 47" src="https://github.com/user-attachments/assets/69e9e2b5-a161-46b1-b66d-228a7b78d62c">

<br>

> ③ Python code

<br>

```python
# np.linalg.svd computes the SVD of a matrix
A = np.array([[4,11,14],
            [8,7,-2]])
U, s, Vt=np.linalg.svd(A)
print(U)
print(s) # s is the vector of nonzero singular values
print(Vt)
```

<br>

⑶ **Application 1.** A<sup>T</sup>A = V(∑<sup>T</sup>∑)V<sup>T</sup> : The singular values of A are the square roots of the eigenvalues of A<sup>T</sup>A. However, some eigenvalues are set to 0 according to the rank.

⑷ **Application 2.** reduced SVD: Note that for the reduced version, both U and V have orthonormal columns. This means that U<sup>T</sup>U = I and V<sup>T</sup>V = I. However, U and V are not square in this version, so they are not orthogonal matrices.

<br>

![image](https://github.com/user-attachments/assets/ac360053-c3ee-48e0-aa9c-8df67c0de893)

<br>

```python
A = np.array([[4,11,14],
            [8,7,-2]])
U, s, Vt=np.linalg.svd(A, full_matrices=False)
print(U)
print(s)
print(Vt)
```

<br>

⑸ **Application 3.** rank-k approximation

> ① For k < rank(A), the m × n matrix A can be split into m × k + k × n matrices to achieve compression.

<br>

![image](https://github.com/user-attachments/assets/1c811701-96a9-4742-8fd9-754983d8fe7d)

<br>

② Given k, how to find the optimal rank-k approximation of A:

> ○ A = UΣV<sup>T</sup> = σ<sub>1</sub>**u**<sub>1</sub>v<sub>1</sub><sup>T</sup> + ··· + σ<sub>r</sub>**u**<sub>r</sub>**v**<sub>r</sub><sup>T</sup>, where σ<sub>1</sub> ≥ ⋯ ≥ σ<sub>r</sub>, remove the smaller singular values σ<sub>r</sub>, σ<sub>r−1</sub>, ….

> ○ By doing this, the Frobenius distance between A<sup>(k)</sup> and A is defined as follows:

<br>

![image](https://github.com/user-attachments/assets/570427b5-9fee-4986-8b08-a38539a9d781)

<br>

<br>
 
## **6\. Type 5.** Independent Component Analysis (ICA)

⑴ Unlike PCA, it reduces dimensionality by statistically separating multivariate signals into independent components.

⑵ The distribution of independent components follows a non-normal distribution.

<br>

<br>
 
## **7\. Others**

⑴ Factor analysis

> ① Definition **:** A method of interpreting the structure within the data by grouping similar variables based on their correlations.

> ② Assumes the existence of latent variables that cannot be observed in the data.

> ③ Often used in social sciences or surveys.

⑵ Multi-dimensional scaling (MDS)

> ① Definition **:** A statistical technique for visualizing proximity and grouping of objects in a lower-dimensional space by identifying patterns and structures latent in the data.

> ② Measures the similarities and dissimilarities between objects and represents them as points in a 2D or 3D space.

⑶ Linear discriminant analysis (LDA)

> ① Definition **:** A method that reduces dimensionality by projecting data onto a lower-dimensional space, similar to PCA.

⑷ CCA (Canonical-correlation analysis) **:** Used in the [Seurat pipeline](https://jb243.github.io/pages/1788).

⑸ RPCA (Reciprocal principal component analysis) **:** Used in the [Seurat pipeline](https://jb243.github.io/pages/1788).

⑹ PHATE

⑺ ODA

⑻ LLE

⑼ LSA

⑽ [PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis) (probabilistic latent semantic analysis)

> ① Used to decompose mixed components from the latent class model.

> ② Application : Used for dimension reduction in mass spectrometry data.

<br>

---

_Input: 2021.12.3 16:22_
