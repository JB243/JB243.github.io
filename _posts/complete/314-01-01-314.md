## **Chapter 22. Image Generative Models**

Recommended Reading **:** 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [DIP](#1-dip)

**2.** [Computer Vision Foundation Models](#2-computer-vision-foundation-models)

---

<br>

## **1. [DIP](https://jb243.github.io/pages/2164)** (deep image prior)

 ⑴ Features **:** Overfits the CNN architecture to the input image without training data to generate new images

<br>

<br>

## **2. Computer Vision Foundation Models**

⑴ Vision Transformer (ViT)

> ① ViT uses only the [transformer encoder](https://jb243.github.io/pages/325#:1.-,,-\(transformerencoder\)) structure

> ② **Step 1.** Split the image into multiple small patches and treat each patch as a token for input into the transformer

> ③ **Step 2.** Embed each patch using the transformer encoder

> ④ **Step 3.** Just like embedding words in a sentence and outputting the sentence embedding that represents the sentence's meaning, ViT learns the relationships between patches and outputs features representing the whole image

⑵ Types

> ① DALL·E3 (OpenAI), Midjourney, Stable Diffusion, Sora (OpenAI), video LLM

> ② [CTransPath](https://www.sciencedirect.com/science/article/pii/S1361841522002043) **:** Wang et al., Medical Image Analysis (2022)

> ③ [UNI](https://www.nature.com/articles/s41591-024-02857-3) **:** Chen et al., Nature Medicine (2024)

> ④ [CONCH](https://www.nature.com/articles/s41591-024-02856-4) (CONtrastive learning from Captions for Histopathology) **:** Lu et al., Nature Medicine (2024)

> ⑤ [Virchow](https://arxiv.org/abs/2309.07778) **:** Vorontsov et al., arxiv (2023)

> ⑥ [RudolfV](https://arxiv.org/abs/2401.04079) **:** Dippel et al., arxiv (2024)

> ⑦ [Campanella](https://arxiv.org/abs/2310.07033) **:** Campanella et al., arxiv (2023)

---

<br>

_Input: 2024.04.22 14:08_
