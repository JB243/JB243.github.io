## **Lecture 17. RNN Algorithm**

Recommended Post: 【Algorithm】 [Algorithm Index](https://jb243.github.io/pages/1278)

---

**1.** [Overview](#1-overview)

**2.** [Considerations](#2-considerations)

**3.** **Type 1.** [Regression Models Before RNN](#3-type-1-regression-models-before-rnn)

**4.** **Type 2.** [LSTM](#4-type-2-lstm)

**5.** **Type 3.** [GRU](#5-type-3-gru)

---

<br>

## **1. Overview**

⑴ RNN (recurrent neural network): An algorithm in which the hidden layer has a recursive neural network in a multilayer perceptron consisting of an input layer, hidden layer, and output layer.

⑵ Structure

<br>

![image](https://github.com/user-attachments/assets/42d21caf-6606-4840-a9dc-724626d92c63)

**Figure 1.** Structure of the RNN Algorithm

<br>

⑶ Applications

> ① Natural Language: Uses preceding and following words in text

> ② Speech Signals

> ③ Time-Series Data: Uses past and future values along with the current value

>> Predicting the future by observing changes in data over time is challenging but very important. Such problems can also be solved using deep learning algorithms. By providing multiple training datasets where x(t), x(t - τ), ..., x(t - kτ) are input values and x(t + τ) is the target value, the predicted value y can be expressed as y = f(x(t), x(t - τ), ..., x(t - kτ)). With deep learning algorithms, a network that represents this function f can be created. Not only can a single predicted value be output, but multiple predicted values can also be generated. In this case, **y** = **F**(x(t), x(t - τ), ..., x(t - kτ)) = (x<sub>1</sub>*, ... x<sub>m</sub>*) are the predicted values, and G(t + τ) = (x<sub>1</sub>, ..., x<sub>m</sub>) are the target values.

<br>

<br>

## **2. Considerations**

⑴ **Long-Term Dependency Problem**: The further back data is from the current time, the harder it becomes to process context.

⑵ **Gradient Vanishing (GV)**: A problem where the gradient value converges to 0 as it moves toward the input layer during backpropagation, making updates ineffective.

> ① Solution: Use ReLU instead of the sigmoid activation function.

⑶ **Gradient Exploding (GE)**: A problem where gradients become excessively large, causing abnormal weight updates.

> ① Gradient Clipping: A technique to prevent gradient explosion by setting an upper limit on gradients.

<br>

<br>

## **3. Type 1. Regression Models Before RNN**

⑴ Moving Average Model

<br>

<img width="430" alt="스크린샷 2025-02-12 오후 12 21 30" src="https://github.com/user-attachments/assets/da091936-02de-494f-9db9-697884c1664b" />

<br>

⑵ Autoregressive Model

<br>

<img width="405" alt="스크린샷 2025-02-12 오후 12 21 47" src="https://github.com/user-attachments/assets/611442b9-58aa-4c12-a617-15df974d58d4" />

<br>

⑶ ARMA (Autoregressive Moving Average)

<br>

<img width="300" alt="스크린샷 2025-02-12 오후 12 22 01" src="https://github.com/user-attachments/assets/4063e831-e4d8-4009-93ec-56637bcbe2d9" />

<br>

⑷ ARMAX (Autoregressive Moving Average with Exogenous Inputs): Concerning the external variable x,

<br>

<img width="300" alt="스크린샷 2025-02-12 오후 12 22 15" src="https://github.com/user-attachments/assets/787b9998-3ab0-4c6b-ad38-7d0bf8922f26" />

<br>

⑸ Issues: Assumption of linearity, lack of criteria on how much past data should be used.

<br>

<br>

## **4. Type 2. LSTM**

⑴ LSTM (Long Short-Term Memory): A neural network algorithm designed to address the long-term dependency problem of RNNs.

⑵ Structure: LSTM consists of an input gate, forget gate, and output gate.

> ① **State 1.** Cell State: Information is passed along unchanged.

> ② **State 2.** Forget Gate: If the sigmoid output is 1, the information is retained; if 0, it is discarded.

> ③ **State 3.** Input Gate: Determines which new information will be stored in the cell state.

> ④ **State 4.** Cell State Update: Updates the cell.

> ⑤ **State 5.** Output Gate: Determines the output.

<br>

<br>

## **5. Type 3. GRU**

⑴ GRU (Gated Recurrent Unit): Similar to LSTM but with a simpler structure.

⑵ Structure

<br>

![image](https://github.com/user-attachments/assets/edb46b1e-502c-43e8-8ea3-f2348b2415c5)

**Figure 2.** Structure of GRU

<img width="401" alt="스크린샷 2025-02-12 오후 12 23 15" src="https://github.com/user-attachments/assets/95ac509b-b599-4699-aa63-a43b1ec9cb36" />

<br>

> ① When **r**<sub>t</sub> is close to 0, the intermediate memory unit ignores **h**<sub>t-1</sub>.

> ② When **z**<sub>t</sub> is close to 1, **h**<sub>t</sub> ignores **x**<sub>t</sub> and tries to maintain the **h**<sub>t-1</sub> value.

> ③ Input (Text) - Output (Sentiment: Positive, Negative, Neutral) has hidden layers in between, and the model's performance depends on how these hidden layers are structured.

<br>

---

Input: 2023.06.27 00:35
