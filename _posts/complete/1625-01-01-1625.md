## **Chapter 5. Statistical Quantity**

Higher category **:** 【Statistics】 [Statistics Overview](https://jb243.github.io/pages/1641)

---

**1.** [Expected value](#1-expected-value) 

**2.** [Standard deviation](#2-standard-deviation)  

**3.** [Covariance and correlation coefficient](#3-covariance-and-correlation-coefficient)  

**4.** [Anscombe's quartet](#4-anscombe-s-quartet)  

**5.** [Ordinal statistics](#5-ordinal-statistics)

**6.** [Conditional statistics](#6-conditional-statistics)

---

**a.** [SSIM](https://jb243.github.io/pages/2067) 

**b.** [Distance Function and Similarity](https://jb243.github.io/pages/879) 

---

<br>

## **1. Expected value**

⑴ definition **:** the expected value of the random variable X, _i.e._ E(X), is the X value obtained on average as a result of the implementation

> ① discrete random variable

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdfGhD4%2FbtrhSEADZ1j%2FVLPYckKlPgSN3QiK5LCPAk%2Fimg.png" alt="drawing">
</center>
  <br>

> ② continuous random variable

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fm35qh%2FbtrhUXMrlMt%2FpY6BBs9syVdqCT9fnV727k%2Fimg.png" alt="drawing">
</center>
  <br>
  
⑵ joint probability distribution function

> ① discrete random variable

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FJJc7x%2FbtrhTppdlDW%2F7SigOKv6iXNAsWQyiCrRf0%2Fimg.png" alt="drawing">
</center>
  <br>

> ② continuous probability variable

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc4uJhv%2FbtrhV9ephHB%2FQZBEGMBIZ3vbLtXOQRM0MK%2Fimg.png" alt="drawing">
</center>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FO3HHG%2FbtrhTRlCksM%2Fb6dyyJNsUPCgGjkH34xm0K%2Fimg.png" alt="drawing">
</center>
  <br>

⑶ the properties of expected values

> ① linearity **:** E(aX + bY + c) = aE(X) + bE(Y) + c

> ② if X and Y are **independent**, E(XY) = E(X) × E(Y)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbwrpL9%2FbtrhWaxCo7k%2Fxhbpdq77Eu5bK2Nlhhnvfk%2Fimg.png" alt="drawing">
</center>
  <br>

⑷ example

> ① X**:** if you mix n hats and extract one without-replacement, the number of people who correctly found their hats

> ② problem purpose**:** it is difficult to calculate E(X) after obtaining p(X)

> ③ X = X<sub>1</sub> + ··· + X<sub>n</sub>.  X<sub>i</sub> **:** if i-th person found his hat, the value is 1, if not 0

> ④ **approach 1.** number of cases

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FTbaWh%2FbtrhS37914L%2FZNJpwRmFXzDuAWvAki62nK%2Fimg.png" alt="drawing">
</center>
  <br>

> ⑤ **approach 2.** when the i-th person first extracts or not, the expected value is consistent based on symmetry.

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FYLiNE%2FbtrhS3UDArA%2FD36Veaki43gu4aDT2KD7lk%2Fimg.png" alt="drawing">
</center>
  <br>

⑸ Cauchy distribution**:** the expected value is not defined

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F0G0bI%2FbtrhTg0t0Le%2FjbrsuBWKJvVKR5jjZmjni1%2Fimg.png" alt="drawing">
</center>
  <br>

<br>

## **2. Standard deviation**

⑴ deviation

> ① definition**:** D = X - E(X)

> ② **characteristic 1.** E(D) = E(X - E(X)) = E(X) - E(X) = 0 

⑵ variance

> ① definition**:** when E(X) = μ, VAR(X) = E((X - μ)<sup>2</sup>) = E(D<sup>2</sup>)

> ② **characteristic 1.** VAR(X) = E(X<sup>2</sup>) - μ<sup>2</sup>

>> ○ proof**:** VAR(X) = E((X - μ)<sup>2</sup>) = E(X<sup>2</sup>) - 2μE(X) + μ<sup>2</sup> = E(X<sup>2</sup>) - 2μ<sup>2</sup> + μ<sup>2</sup> = E(X<sup>2</sup>) - μ<sup>2</sup>

> ③ **characteristic 2.** VAR(aX + b) = a<sup>2</sup> VAR(X)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fmc9JX%2FbtrhThkO9Hc%2FNTKBmfFzQgd4Obal8z4QL0%2Fimg.png" alt="drawing">
</center>
  <br>

> ④ **characteristic 3.** introduction to covariance**:** VAR(X + Y) = VAR(X) + VAR(Y) + 2 COV(X, Y)

>> ○ proof

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FNaryv%2FbtrhVtdfvaL%2FIiI8INkXGxRK58RxKFF2tK%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ generalization

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FoTwKb%2FbtrhTRlCmAv%2F8wtmj7zreENNfrKx6MG8rk%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ linearity**:** when X and Y are **independent**, VAR(X + Y) = VAR(X) + VAR(Y)

>> ○ Definition of covariance **:** Given a data set of non-overlapping (x<sub>1</sub>, y<sub>1</sub>), ···, (x<sub>n</sub>, y<sub>n</sub>), the covariance of x and y is given as follows

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FT8InR%2FbtrCsdXj17e%2FzEgKLgIkj9kAJlMs465g11%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ If redundancy is allowed, the definition of covariance is modified as follows by introducing the sample ratio p<sub>i</sub> **:** if y<sub>i</sub> = x<sub>i</sub>, then covariance = variance

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FzhM2m%2FbtrCvW8jAR1%2FkmXoSrbQsRYdey87gGzxw0%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ Two-dimensional covariance matrix Σ (where x = (x<sub>1</sub>, x<sub>2</sub>)<sup>T</sup> = (x, y)<sup>T</sup>)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbamU0L%2FbtrCxGQ9t1T%2FIcKqZeaTikGB30AKhDKXTk%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ Σ = E[(**x**-E[**x**])(**x**-E[**x**])<sup>T</sup>] is established not only for two dimensions but also for n dimensions.

> ⑤ **characteristic 4.** VAR(X) = 0 ⇔ P(X = constant) = 1 (**∵** [Chebyshev inequality](https://jb243.github.io/pages/1594))

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FR5p7H%2FbtrhTR61sz5%2FsjIGk0YUMmJeQVpjkcpuE0%2Fimg.png" alt="drawing">
</center>
  <br>

⑶ standard deviation

> ① definition**:** standard deviation of X, _i.e._ σ or SD(X) = √ VAR(X) ⇔ σ<sup>2</sup> = VAR(X) 

> ② idea**:** X and variance differ in unit, but X and standard deviation are same in unit 

> ③ characteristic**:** variance and σ are always non-negative. covariance can have negative value 

⑷ coefficient of variation

> ① Standard deviation divided by mean

> ② Used to relatively compare the degree of scattering of data with different units of measurement

<br>

<br>

## **3. Covariance and correlation coefficient** 

⑴ covariance 

> ① definition**:** about E(X) = μ<sub>x</sub> , E(Y) = μ<sub>y</sub>, 

<br>

<center>COV(X, Y) = σ<sub>xy</sub> = E｛(X - μ<sub>x</sub>)(Y - μ<sub>y</sub>)｝</center>

<br>

> ② meaning **:** when X changes, the degree of change of Y

> ③ **characteristic 1.** COV(X, Y) = E(XY) - E(X)E(Y)

>> ○ proof **:** COV(X, Y) = E((X - μ<sub>x</sub>)(Y - μ<sub>y</sub>)) = E(XY) - μ<sub>x</sub>E(Y) - μ<sub>y</sub>E(X) + μ<sub>x</sub>μ<sub>y</sub> = E(XY) - μ<sub>x</sub>μ<sub>y</sub>

> ④ **characteristic 2.** if X = Y, COV(X, Y) = VAR(X)

> ⑤ **characteristic 3.** if X and Y are independent, COV(X, Y) = 0

>> ○ proof**:** COV(X, Y) = E(XY) - E(X)E(Y) = E(X)E(Y) - E(X)E(Y) = 0

>> ○ because independence is a more stringent condition, even if COV (X, Y) = 0, it is not possible to conclude that X and Y are independent

> ⑥ **characteristic 4.** COV(aX + b, cY + d) = ac COV(X, Y)

> ⑦ **characteristic 5.** COV(a<sub>1</sub> X<sub>1</sub> + a<sub>2</sub> X<sub>2</sub>, Y) = a<sub>1</sub> COV(X<sub>1</sub>, Y) + a<sub>2</sub> COV(X<sub>2</sub>, Y)

> ⑧ limitation**:** by characteristic 4, covariance contains both association and size information, so you cannot say only association 

⑵ correlation coefficient**:** also referred to as Pearson correlation coefficient

> ① definition**:** about standard deviation X and Y, _i.e._ σ<sub>x</sub>, σ<sub>y</sub> each, 

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwrIKo%2FbtrhVsZHDg6%2F1UmbBKKLUONctr6fxYj9Kk%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ Multiple correlation coefficients: the representation of correlation coefficients when there are three or more variables

>> ○ Complete correlation: ρ = 1

>> ○ No correlation: ρ = 0

> ② background**:** to show only association information except size information. related to the limitation of covariance 

> ③ Characteristics

>> ○ Correlation between two variables measured on an interval or ratio scale.
   
>> ○ Targeted towards continuous variables.
   
>> ○ Assumption of normality.
   
>> ○ Widely utilized in most cases.
     
> ④ **characteristic 1.** -1 ≤ ρ(X, Y) ≤ 1 (correlation inequality)

>> ○ proof**:** [Coshi-Schwarz inequality](https://jb243.github.io/pages/1594) 

>> ○ ρ(X, Y) = 1**:** X and Y are fully proportional

>> ○ ρ(X, Y) = -1**:** complete inverse relationship of X and Y

>> ○ ρ(X, Y) = 0 does not mean X and Y are independent

>> ○ **exception** **1.** p(x) = ⅓ _I_｛x = -1, 0, 1｝ , Y = X<sup>2</sup>

>>> ○ COV(X, Y) = E(XY) - E(X)E(Y) = E(XY) - E(X3) = 0 

>>> ○ because p(1, 1) = ⅓, p(x = 1) = ⅓, p(y = 1) = ⅔, p(x, y) ≠ p(x) × p(y) 

>>> ○ disagreements in the definition of independence

>> ○ <span> <b>exception 2.</b> S =｛(x, y) | -1 ≤ x ≤ 1, x<sup>2</sup> ≤ y ≤ x<sup>2</sup> + 1/10｝, p = 5 _I_ ｛(x, y) ∈ S｝ </span>

>>> ○ COV(X, Y) = E(XY) - E(X)E(Y) = E(XY) = 0  

>>> ○ in the definition of independence, constant = p(x, y) = p(x) × p(y) should be met. however, p(y) is not constant

>>> ○ disagreements in the definition of independence

> ⑤ **characteristic 2.** ρ(X, X) = 1, ρ(X, -X) = -1

> ⑥ **characteristic 3.** ρ(X, Y) = ρ(Y, X)

> ⑦ **characteristic 4.** exclusion of size information**:** ρ(aX + b, cY + d) = ρ(X, Y)

>> ○ proof**:** ρ(aX + b, cY + d) = COV(aX + b, cY + d) ÷ aσx ÷ cσy = COV(X, Y) ÷ σxσy = ρ(X, Y)

> ⑧ <span><b>characteristic 5.</b> association information <b>:</b> | ρ(X, Y) | = 1 and Y = aX + b, (a ≠ 0, b constant) are necessary and sufficient condition </span>

>> ○ proof of forward direction**:** The idea of setting Z comes from [simple regression analysis](https://jb243.github.io/pages/1632)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbjWOAN%2FbtrhS31o3i7%2F6wN0ZFhyxzbLCgL5blvf21%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ proof of reward direction 

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FOu8c8%2FbtrhUjWwnsm%2Fb2CSyXkWirPqhyRxcq2Kv0%2Fimg.png" alt="drawing">
</center>
  <br>

> ⑨ [statistical estimation of correlation coefficient](https://jb243.github.io/pages/1630)

>> ○ null hypothesis H<sub>0</sub> **:** correlation coefficient = 0

>> ○ alternative hypothesis H<sub>1</sub> **:** correlation coefficient ≠ 0

>> ○ calculation of t statistics**:** about the correlation coefficient r obtained from the sample,

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbDhHsw%2FbtrpPESUjco%2FMmP6sUGHE4wZKxA1zZV8l1%2Fimg.jpg" alt="drawing">
</center>
  <br>

>> ○ the above statistics follow the student t distribution with a degree of freedom of n-2 (assuming the number of samples is n)

> ⑩ [calculation in R Studio](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#:~:text=Compute%20correlation%20in%20R-,R%20functions,-Correlation%20coefficient%20can) 

>> ○ cor(x, y)

>> ○ cor(x, y, method = "pearson")

>> ○ cor.test(x, y)

>> ○ cor.test(x, y, method = "pearson")

⑶ Spearman correlation coefficient

> ① definition**:** about x' = rank(x) and y' = rank(x), 

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FSWYEG%2FbtrpOwHCN1U%2F0aw49KnBM6dU7b4JMrF2B0%2Fimg.png" alt="drawing">
</center>
  <br>

> ② Characteristics

>> ○ A method of measuring the correlation between two variables that are in ordinal scale.
   
>> ○ A non-parametric method targeting ordinal variables.
   
>> ○ Advantageous in data with many ties (zeroes).

>> ○ Sensitive to deviations or errors within the data.
   
>> ○ Tends to yield higher values than Kendall's correlation coefficient.

> ③ [calculation in R Studio](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#:~:text=Compute%20correlation%20in%20R-,R%20functions,-Correlation%20coefficient%20can) 

>> ○ cor(x, y, method = "spearman")

>> ○ cor.test(x, y, method = "spearman")

⑷ Kendall correlation coefficient

> ① definition**:** defined about concordant pair and discordant pair

> ② Characteristics

>> ○ A method of measuring the correlation between two variables that are in ordinal scale.
   
>> ○ A non-parametric method targeting ordinal variables.
   
>> ○ Advantageous in data with many ties (zeroes).
   
>> ○ Useful when the sample size is small or when there are many tied values in the data.

> ③ Procedure

>> ○ **step 1.** sort y values in ascending order for x values

>> ○ **step 2.** for each  y<sub>i</sub>, count the number of concordant pairs in which y<sub>j</sub> ＞ y<sub>i</sub> (assuming j ＞ i)

>> ○ **step 3.** for each y<sub>i</sub>, count the number of discordant pairs in which y<sub>j</sub> ＜ y<sub>i</sub> (assuming j ＞ i)

>> ○ **step 4.** define correlation coefficient as follows:

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FNspU2%2FbtrpSmqeCR8%2FSaNluz2GBPbTq26m7P5sEK%2Fimg.png" alt="drawing">
</center>
  <br>

>>> ○ n<sub>c</sub> **:** total number of concordnat pairs 

>>> ○ n<sub>d</sub> **:** total number of discordant pairs 

>>> ○ n **:** size of x and y

> ④ [calculation in R Studio](http://www.sthda.com/english/wiki/correlation-test-between-two-variables-in-r#:~:text=Compute%20correlation%20in%20R-,R%20functions,-Correlation%20coefficient%20can) 

>> ○ cor(x, y, method = "kendall")

>> ○ cor.test(x, y, method = "kendall")

⑸ χ<sup>2</sup> **:** A measure of the suitability of the approximation

> ① If the measurement data is x<sub>m</sub>, y<sub>m</sub>, and the approximate function is f(x)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdcD0JL%2FbtrILe9f7PV%2FkkQoEJePOFoT0pVCspLZOk%2Fimg.png" alt="drawing">
</center>
  <br>

> ② Calculating the infinitesimal point through the differential of χ<sup>2</sup> when obtaining an approximate function.

> ③ Use in [non-linear regression](https://jb243.github.io/pages/1633) such as quadratic approximation function

<br>

<br>

## **4. Anscombe's quartet**  

⑴ showing that the mean, standard deviation, and correlation coefficient cannot describe the shape of a given data

⑵ **example 1**

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FR11mi%2FbtrnBAdTsjk%2FwX2c7WRl3SlwvOP4L42yLk%2Fimg.png" alt="drawing">
</center>
  <br>

<center><b>Figure. 1.</b> example of Anscombe's quartet</center>

<br>

⑶ **example 2**

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcCrYI4%2FbtrtKDQmh68%2FFpVkyagPjCvvpHyM4rTzfK%2Fimg.png" alt="drawing">
</center>
  <br>

<center><b>Figure. 2.</b> 2nd example of Anscombe's quartet</center>

<br>

<br>

## **5. Ordinal statistics**

⑴ assumption **:** X<sub>i</sub> and X<sub>j</sub> are independent 

⑵ definition **:** set Y<sub>i</sub> to be Y<sub>1</sub> ＜ ··· ＜ Y<sub>n</sub> by rearranging X<sub>1</sub>, ···, and X<sub>n</sub> 

⑶ joint probability distribution

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLRr5P%2FbtrhUXMrqpX%2FGRKcG24aIz94fGpZbxLCxK%2Fimg.png" alt="drawing">
</center>
  <br>

⑷ marginal probability distribution

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbU9I8w%2FbtrhVrGvAtB%2FG95DaLWF2Sy4RnjKKTki2K%2Fimg.png" alt="drawing">
</center>
  <br>

⑸ expected value

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlQgCX%2FbtrhThryA8X%2F7eVj9M7WXzgkgskuXrYdeK%2Fimg.png" alt="drawing">
</center>
  <br>

<br>

## **6. Conditional statistics**

⑴ conditional expectation

> ① definition

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FuQ3Nb%2FbtrhTR0fFc5%2FKs3q1d5n50gKroy9lrFIE0%2Fimg.png" alt="drawing">
</center>
  <br>

> ② characteristic

>> ○ <span>E(XY | Y) = YE(X | Y)</span>

>> ○ <span>E(aX<sub>1</sub> + bX<sub>2</sub> | Y) = aE(X<sub>1</sub> | Y) + b(X<sub>2</sub> | Y)</span>

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FUZZ3j%2FbtrhVsSXTNo%2FXO2rp1rkZ7oLmVMIg72G8K%2Fimg.png" alt="drawing">
</center>
  <br>

> ③ law of iterated expectation

>> ○ lemma

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnvjpT%2FbtrhThd0d2t%2FTmM3YGkXKCshsjFgPHklT1%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ proof

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FJvOLx%2FbtrhSM6FkOf%2FfsDMPxHjYhW9JuuaCr04x0%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ example

>>> when selecting a point of Y randomly at［0, ℓ］ as a uniform distribution, and then a point of X randomly at ［0, y］ as a uniform distribution,

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F4a5sQ%2FbtrhTSkxZVh%2FsGqOlT2VeJv95GAlnQzRf0%2Fimg.png" alt="drawing">
</center>
  <br>

> ④ mean independence

>> ○ independence ⊂ mean independence ⊂ uncorrelatedness

>> ○ average independence

>> ○ uncorrelatedness**:** if the correlation coefficient is 0

>> ○ normal distribution**:** if X and Y are jointly normal and uncorrelated, then X and Y are independent

> ⑤ [simple regression analysis](https://jb243.github.io/pages/1632)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcJsBIC%2FbtrhSMewH2E%2FgJfoOaxRrCN8F0E43eZ8HK%2Fimg.png" alt="drawing">
</center>
  <br>

⑵ conditional variance

> ① definition**:** the conditional variance of Y for a given probability variable X

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FckRgQY%2FbtrhVtdfCF1%2FCmctIjySsYYdYpbuvifr30%2Fimg.png" alt="drawing">
</center>
  <br>

> ② law of total variance (decomposition of variance)

>> ○ lemma

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbNoj98%2FbtrhWH29tXM%2FDW6r6DgXdcJmwQ01jr9UXk%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ proof

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbfqBaL%2FbtrhThLSBJq%2FrCsDBihH8I5dl7v7fk30Zk%2Fimg.png" alt="drawing">
</center>
  <br>

>> ○ meaning

>>> ○ situation**:** when X ~ P<sub>1</sub>(θ), Y ~ P<sub>2</sub>(X) 

>>> ○ <span>use P<sub>2</sub> to calculate VAR(Y | X) and E(Y | X) </span>

>>> ○ use P<sub>1</sub> to calculate E｛·｝, VAR｛·｝

>>> ○ <span>E(VAR(X | Y))</span> **:** intra-group variance

>>> ○ <span>VAR(E(X | Y))</span> **:** inter-group variance

>> ○ example

>>> ○ X **:** laid-off worker's unemployment period 

>>> ○ probability density function of X**:** [exponential distribution](https://jb243.github.io/pages/1627)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FolOA0%2FbtrhWJ0YeSp%2FvrkPfRVKf013eD1v57md9k%2Fimg.png" alt="drawing">
</center>
  <br>

>>> ○ 20% of the total workforce**:** skilled labor force. λ = 0.4

>>> ○ 80% of the total workforce**:** unskilled workers. λ = 0.1

>>> ○ calculation of VAR(X)

  <br>
<center>
<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FHapes%2FbtrhS3AlmDH%2FXABrLoCkHKkui9UCYL7cw1%2Fimg.png" alt="drawing">
</center>
  <br>

---

*Input : 2019.06.17 14:15*
