## **Chapter 20. Information Theory**

Higher category **:** 【Statistics】 [Statistics Table of Contents](https://jb243.github.io/pages/1641)

---

**1.** [Information Theory](#1-information-theory)

**2.** [Modern Information Theory](#2-modern-information-theory)

---

<br>

## **1. Information Theory** 

⑴ Overview

> ① low probability event: high information (surprising)

> ② high probability event: low information (unsurprising)

⑵ **entropy**

> ① Problem situation: X is Raining / Not Raining. Y is Cloudy / Not Cloudy

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/23a759e7-51ef-4b02-922f-585087efe8ff)

**Figure. 1.** Entropy example

<br>

> ② Basic concepts of statistics

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/89d6a97a-2c89-46b0-8717-66b696decd1f)

<br>

> ③ **information**: everything obtained when the result is known

> ④ **uncertainty** (surprisal, unexpectedness, randomness)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/c9dfd579-b7d0-4544-b369-4881c843b2ca)

<br>

>> ○ **Definition 1.** Absence of information about the outcome

>> ○ **Definition 2.** Number of bits required to represent the given number of cases

>>> ○ When the base is 2, the unit used is **bits**

>>> ○ When the base is e (natural logarithm), the unit used is **nats**

> ⑤ **entropy** (self information)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/7f9003f6-58d6-4842-bfed-deba521808a1)

<br>

>> ○ Understanding

>>> ○ In other words, entropy is defined as the average value of uncertainty

>>> ○ Similar to entropy in physics, higher values are obtained as disorder increases: higher entropy means higher heterogeneity

>>> ○ It applies the surprisal (-log p(x)) of each event to the probability weight (p(x)) of each event

>> ○ Examples

>>> ○ Fair coin toss **:** H = -0.5 log2 0.5 - 0.5 log2 0.5 = 1

>>> ○ Fair die ****: H = 6 × (-(1/6) log2 (1/6)) = log2 6 = 2.58

>>> ○ If X follows a uniform distribution, H(X) = log n holds

>> ○ **cross entropy**

>>> ○ Definition: Average number of bits required to distinguish two probability distributions, p and q

>>> ○ In machine learning, model prediction

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/12fa0d20-453d-45ad-b404-4a1c06a4c9d9)

<br>

>> ○ **Feature 1.** H(X) is shift-invariant

>>> ○ When Y = X + a, H(Y) = H(X) is established because p<sub>Y</sub>(y) = p<sub>X</sub>(x)

>> ○ **Feature 2.** H(X) ≥ 0

>> ○ **Feature 3.** H<sub>b</sub>(X) = (log<sub>b</sub>a) H<sub>a</sub>(X) (where a and b are under the log)

>>> ○ ○ log<sub>b</sub>p = (log<sub>b</sub>a) log<sub>a</sub>p

> ⑥ **joint entropy**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/3b3393cf-298e-43fa-8836-81c501a1f09a)

<br>

>> ○ **Feature 1.** If X and Y are independent, H(X, Y) = H(X) + H(Y)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/06ca76b6-fa1d-4109-8efa-0b2a07bfe860)

<br>

> ⑦ Conditional entropy 

>> ○ Conditional entropy for a specific condition

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/4bf0ab45-5340-4d5e-a160-630cf5b57b7b)

<br>

>> ○ Total conditional entropy

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/63809a8e-3f33-4df1-8ee1-0717f39856bd)

<br>

>> ○ Information gain (IG): The increase in order when knowing the information X

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/e21fa1cf-4b6d-464f-8d92-0dd9980ce694)

<br>

>> ○ Information bottleneck method (IB) **:** Describing the trade-off between complexity and accuracy using information theory

>>> ○ Application **:** For example, it can determine the number of clusters in given data.

>> ○ **Feature 1.** <span>If X and Y are independent, H(Y | X) = H(Y)</span>

>> ○ **Feature 2.** <span>H(Y | Y) = 0</span>

>> ○ **Feature 3.** <span>H(X, Y) = H(X) + H(Y | X) = H(Y) + H(X | Y) (Chain rule)</span>

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/494f69e9-dcf0-45cb-ba19-13cdae3221dc)

<br>

>> ○ **Feature 4.** <span>H(X, Y | Z) = H(X | Z) + H(Y | X, Z)</span>

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/405aef03-68fd-46d7-9277-f140503602cd)

<br>

> ⑧ Relative entropy ([Kullback Leibler distance](https://jb243.github.io/pages/1140), KL distance)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/13798be3-1df6-493b-95a0-508ebe9316d8)

<br>

>> ○ Overview

>>> ○ A measure of how two probability distributions diverge.

>>> ○ It indicates the divergence between an approximate distribution p(x), which aims to mimic the true distribution, and the true distribution q(x).

>>> ○ Typically, the true distribution is a complex probability distribution function, while the approximate distribution is represented using a parameterized model.

>>> ○ The approximate distribution can be made manageable, often resembling a simple distribution like the normal distribution.

>> ○ Mutual information

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/4294cf3b-9475-49d6-995b-f7032c623824)

<br>

>>> ○ Information gain and mutual information are equivalent.

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/4de8e5e8-b3c7-49ca-a642-1f540f95c370)

<br>

>>> ○ Mutual information can be used for assessing image [similarity](https://jb243.github.io/pages/879).

>> ○ **Feature 1.** asymmetry **:** D<sub>KL</sub>(p(x) || q(x)) ≠ D<sub>KL</sub>(q(x) || p(x))

>> ○ **Feature 2.** <span>D(p || q) ≥ 0 (equality condition: p(x) = q(x))</span>

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/665993f3-223f-42b6-b518-bf4f0135c880)

<br>

>> ○ **Feature 3.** Mutual information ≥ 0 (equality condition: X and Y are independent)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/c3a9303e-aea7-4cca-ae82-5b1a67fcd325)

<br>

>> ○ **Feature 4.** H(X) ≤ log n (where n is the number of elements X can take)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/4c86d19c-26f3-4a5b-ba97-8ba6d17ef9e2)

<br>

>> ○ **Feature 5.** <span>H(X | Y) ≤ H(X)</span> (equality condition: X and Y are independent)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/9a01b7a1-7ee7-4cfd-ab1c-d599685c5619)

<br>

>> ○ **Application.** Determining the predictive data distribution p with the smallest KL distance to the true unknown data distribution q.

>>> ○ Maximizing log-likelihood minimizes KL distance.

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/98e9cba1-b218-45cf-a217-3dc68a2408e5)

<br>

> ⑨ Asymptotic equipartition property (AEP)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/4d9d0ac1-ca5a-4f54-abd8-0a34d4c0ff9b)

<br>

⑶ **Comparison 1.** `Gini impurity`

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/2f23f80f-3e3c-4dee-a6bd-b286cf88dc99)

<br>

> ① Entropy is not the unique measure, alternative measures like Gini impurity are proposed.

> ② Gini impurity considers the probability of mislabeling (1 - P(x<sub>i</sub>)) weighted by the weight of each sample (P(x<sub>i</sub>)).

> ③ Both Gini impurity and entropy increase as the given data becomes more disordered: Origin of impurity.

> ④ (Conceptual Distinction) The Gini Coefficient in Economics

>> ○ The concept was first introduced to evaluate income inequality: It can assess not only income but also the degree of inequality in the distribution of a discrete variable.

>> ○ As the Gini coefficient moved from economics to the fields of machine learning and information theory, it implies similar inequality, but the mathematical definition changes.

>> ○ Difference: If the distribution of the variable is entirely uniform,

>>> ○ In economics, the Gini coefficient becomes extremely egalitarian, so it has a value of 0.

>>> ○ In information theory, the Gini coefficient attains its maximum value due to the Cauchy-Schwarz inequality. Intuitively, it becomes most disordered, thus reaching the maximum value.

⑷ **Comparison 2.** `Connectivity index (CI)`

> ① In a spatial lattice, if a specific spot and its nearest neighbor are in different clusters, the CI value increases.

> ② Therefore, a higher CI value implies higher heterogeneity.

⑸ **Comparison 3.** `Simpson index`

> ① Another measure based on information theory, along with Shannon entropy

> ② Based on spatial transcriptomics, the probability of two random spots belonging to the same cluster ([ref](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01185-4#Sec2))

> ③ The lower the Simpson index, the higher the heterogeneity implied

⑹ **Comparison 4.** `Cluster modularity`

> ① Based on spatial transcriptomics, a metric indicating the purity (homogeneity) of spots within a cluster ([ref](https://genomemedicine.biomedcentral.com/articles/10.1186/s13073-023-01185-4#Sec2))

⑺ **Comparison 5.** `Lempel-Ziv complexity`

⑻ **Comparison 6.** Silhouette coefficient

⑼ **Comparison 7.** Calinski-Harabasz index

⑽ **Application 1.** Markov process

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/3f5f81d2-e4b9-444b-8243-da5da7e63abb)

<br>

> ① two-state Markov chain

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/08815509-5516-424a-ab28-17d500d935a2)

![image](https://github.com/JB243/jb243.github.io/assets/55747737/5f028399-b20f-475f-9a58-5973c0d4dd04)

<br>

**Figure. 2.** two-scale Markov chain

<br>

> ② Hidden Markov model

>> ○ If χ = {X<sub>i</sub>} is a Markov process and Y<sub>i</sub> = ϕ(X<sub>i</sub>) (where ϕ is a deterministic function), then y = {Y<sub>i</sub>} is a hidden Markov model.

> ③ **Feature 1.** If <span>Pr(x<sub>n</sub> | x<sub>n-1</sub>)</span> is independent of n, the Markov process is stationary (time-invariant).

> ④ **Feature 2.** Aperiodic ⊂ irreducible.

> ⑤ **Feature 3.** Markov process can prove the second law of thermodynamics (law of increasing entropy).

>> ○ Assumes uniform stationary distribution.

<br>

<br>

## **2. Modern Information Theory** 

⑴ History

> ① Ludwig Boltzmann (1872) **:** Proposed the H-theorem to explain the entropy of gas particles.

> ② Harry Nyquist (1924) **:** Quantified the speed at which "intelligence" is propagated through a communication system.

> ③ John von Neumann (1927): Extended Gibbs free energy to quantum mechanics.

> ④ Ralph Hartley (1928): Expressed the number of distinguishable information by using logarithms.

> ⑤ Alan Turing (1940): Introduced deciban for decrypting the German Enigma cipher.

> ⑥ Richard W. Hamming (1947): Developed Hamming code.

> ⑦ Claude E. Shannon (1948): Pioneered information theory. [A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)

> ⑧ Solomon Kullback & Richard Leibler (1951): Introduced Kullback-Leibler divergence

> ⑨ David A. Huffman (1951): Developed Huffman encoding.

> ⑩ Alexis Hocquenghem & Raj Chandra Bose & Dwijendra Kumar Ray-Chaudhuri (1960): Developed BCH code.

> ⑪ Irving S. Reed & Gustave Solomon (1960): Developed Reed-Solomon code.

> ⑫ Robert G. Gallager (1962): Introduced low-density parity check.

> ⑬ Kolmogorov (1963): Introduced Kolmogorov complexity (minimum description length).

> ⑭ Abraham Lempel & Jacob Ziv (1977): Developed Lempel-Ziv compression (LZ77).

> ⑮ Claude Berrou & Alain Glavieux & Punya Thitimajshima (1993): Developed Turbo code.

> ⑯ Erdal Arıkan (2008): Introduced polar code.

⑵ **Type 1.** Encryption

> ① Huffman code

> ② Shannon code

> ③ Shannon-Fano-Elias code (alphabetic code)

⑶ **Type 2.** Compression

> ① Uniquely decodable (UD)

> ② Kraft inequality

⑷ **Type 3.** Network information theory

⑸ **Type 4.** Quantum information theory

<br>

---

_*Input: 2021-11-10 22:28*_

_*Modified: 2023-03-21 16:22*_
