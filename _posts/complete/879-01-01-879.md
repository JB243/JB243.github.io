## **Chapter 5-2. Types of Distances and Similarities**

Recommended Article **:** 【Statistics】 Chapter 5. [Statistics](https://jb243.github.io/pages/1625)

---

**1.** [Overview](#1-overview)

**2.** [Types of Distance Concepts](#2-types-of-distance-concepts)

**3.** [Types of Similarity Concepts](#3-types-of-similarity-concepts)

---

<br>

## **1\. Overview**

 ⑴ The concepts of distance and similarity are often used interchangeably.

> ① Commonality: Saying two data points are close (short distance) is equivalent to saying they are similar. In other words, distance ∝ 1 / similarity.

> ② Commonality: In machine learning, terms like loss function, error function, and cost function also refer to the difference between the true value and the predicted value (∝ 1 / similarity).

> ③ Difference **:** While a distance function is rigorously defined in [linear algebra](https://jb243.github.io/pages/1897), it's not necessary for loss functions or similarity measures to satisfy that definition.

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/5c983f43-6311-4454-b288-562d04d67607)

<br>

⑵ The distinction between the following distance concepts and similarity concepts is not rigorous.

⑶ Various types of distances and concepts of similarity.

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/e4520fbe-f0d0-447a-80a3-c0314a6d3db6)

**Figure. 1.** Various types of distances and concepts of similarity

<br>

<br>

## **2\. Types of Distance Concepts**

 ⑴ **Type 1.** [L1 Loss Function](https://jb243.github.io/pages/1140) (L1-norm)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/258fe8dc-eee6-4a16-8b2e-7ff145431fd4)

<br>

> ① **1-1.** Manhattan Distance: A method of calculating distance by setting paths in shapes like 'ㄱ' and 'ㄴ'.

 ⑵ **Type 2.** [L2 Loss Function](https://jb243.github.io/pages/1140) (L2-norm, MSE) **:** Euclidean distance using the Pythagorean theorem (**standard**)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/8554dcb8-0dbb-4909-895a-342798c9d1a9)

<br>

 ⑶ **Type 3.** [Cross Entropy](https://jb243.github.io/pages/1140) **:** Typically has a binary cross-entropy (BCE).

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/f841e5c5-3f2d-4466-ae9d-9ef45d5bdc65)

<br>

 ⑷ **Type 4.** [KL Distance](https://jb243.github.io/pages/1140)(Kullback-Leibler divergence, KLD)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/1362d9b1-9cb9-492d-9664-5b280287da5d)

<br>

 ⑸ **Type 5.** [Delaunay Triangulation](https://jb243.github.io/pages/1140)(Delaunay triangulation)

 ⑹ **Type 6.** p-norm

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/6bada69a-206f-4909-aae5-bb9d9402f481)

<br>

 ⑺ **Type 7.** Dot Product **:** Vector inner product

 ⑻ **Type 8.** [Linkage Metric](https://jb243.github.io/pages/2150) **:** Defines distance between clusters

⑼ **Type 9. Hamming Distance**: Assigns a binary value to each data point and measures the distance of the data point as the difference in values.

⑽ **Type 10. Standardized Distance**:

> ① Distance standardized by the measurement unit of the variable.

> ② Formula:

>> d(i, j)<sup>2</sup> = (X<sub>i</sub> - X<sub>j</sub>)<sup>T</sup> D<sup>-1</sup> (X<sub>i</sub> - X<sub>j</sub>) 

>> ○ X<sub>i</sub> **:** Starting point matrix

>> ○ X<sub>j</sub> **:** Endpoint matrix

>> ○ D **:** Sample variance (diagonal) matrix

⑾ **Type 11.** Mahalanobis Distance**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/8a17bffc-68eb-4915-b6e5-f93eb501e980)

**Figure. 2.** Mahalanobis Distance

<br>

> ① A statistical distance that considers both the standardization of variables and the correlation between variables (shape of the data distribution).

> ② Formula **:** When trying to determine the distance d between two data points X<sub>i</sub> and X<sub>j</sub>, the following formula is used:

>> d(i, j)<sup>2</sup> = (X<sub>i</sub> - X<sub>j</sub>)<sup>T</sup> S<sup>-1</sup> (X<sub>i</sub> - X<sub>j</sub>) 

>> ○ X<sub>i</sub> : Starting point matrix

>> ○ X<sub>j</sub> : Endpoint matrix

>> ○ S : Sample covariance matrix

> ③ Advantages: Unlike Euclidean distance, it is scale-free, considers data correlation, and has benefits such as outlier detection.

> ④ Limitations: Assumes the normality of the data. The process of calculating the sample covariance matrix is computationally intensive.

> ⑤ Python code

<br>

```python
import numpy as np
from scipy.linalg import inv

def mahalanobis_distance(x, y, covariance_matrix, regularization=1e-6):
    # Add regularization to the covariance matrix's diagonal
    regularized_cov = covariance_matrix + np.eye(covariance_matrix.shape[0]) * regularization
    
    x_minus_y = np.array(x) - np.array(y)
    covariance_matrix_inv = inv(regularized_cov)
    distance = np.dot(np.dot(x_minus_y, covariance_matrix_inv), x_minus_y.T)
    return np.sqrt(distance)

# Example usage:
x = [1, 2, 3]
y = [4, 5, 6]
data = np.array([x, y])
cov_matrix = np.cov(data, rowvar=False)  # Here, we're just using the covariance of x and y for simplicity

print(mahalanobis_distance(x, y, cov_matrix))
```

<br>

⑿ **Type 12.** Levenshtein Distance **:** An algorithm that determines how similar two strings, A and B, are to each other.

⒀ **Type 13.** Minkowski Distance

> ① Distance in m-dimensional Minkowski space.

> ② When m = 1, it is equivalent to Manhattan distance.

> ③ When m = 2, it is equivalent to Euclidean distance.

⒁ **Type 14.** Hausdorff Distance

> ① Formalization: For two sets A = {a<sub>1</sub>, ..., a<sub>p</sub>} and B = {b<sub>1</sub>, ..., b<sub>q</sub>},

>> H(A, B) = max(h(A, B), h(B, A)) 

> ② directed Hausdorff distance: The distance between the two points in A and B that are furthest apart,

>> h(A, B) = max<sub>a ∈ A</sub> min<sub>b ∈ B</sub> <span>|</span><span>|</span> a - b <span>|</span><span>|</span>

⒂ **Type 15.** Focal Loss

> ① Formalization

>> FL = -(1 - P<sub>t</sub>)<sup>γ</sup> log (P<sub>t</sub>)

⒃ **Type 16.** Sørensen–Dice Coefficient (Dice Distance)

> ① Formalization

>> 2 <span>|</span> A ∩ B<span>|</span> / <span>|</span> A ∪ B <span>|</span>

⒄ **Type 17.** [Gromov-Wasserstein distance](https://jb243.github.io/pages/2386) (Kantorovich–Rubinstein metric, Earth Mover's Distance, EMD)

⒅ **Type 18.** Sinkhorn divergence

⒆ **Type 19.** Cressie-Read power divergence

⒇ **Type 20.** Jensen-Shannon distance

⒇ **Type 20.** total variation (TV) distance

⒇ **Type 21.** Kolmogorov-Sminrov distance

⒇ **Type 22.** Hellinger distance **:** Requires kernel density estimation for probability density function (pdf).

⒇ **Type 24.** Huber loss function

⒇ **Type 25.** Bhattacharyya loss 

⒇ **Type 26.** evidence lower bound (ELBO)

⒇ **Type 27.** Aitchison distance **:** Concept of distance in a simplex.

<br>

<br>

## **3\. Types of Similarity Concepts**

⑴ **Type 1.** [Pearson Correlation Coefficient](https://jb243.github.io/pages/1625)(Pearson correlation coefficient)

> ① Given the standard deviations σx, σy of X and Y,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/548d2488-84c5-43c6-964d-a978319b4701)

<br>

⑵ **Type 2.** [Spearman Correlation Coefficient](https://jb243.github.io/pages/1625)(Spearman correlation coefficient)

> ① Define x' = rank(x) and y' = rank(x),

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/0d73fc1a-8a45-4a4d-84aa-3fca86043525)

<br>

⑶ **Type 3.** [Kendall Correlation Coefficient](https://jb243.github.io/pages/1625#)(Kendall correlation coefficient)

> ① Define correlation for concordant and discordant pairs,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/c3d3f7ef-87f7-4c11-81e4-84723d0f58c7)

<br>

⑷ **Type 4.** Matthew correlation coefficient (MCC)

<br>

<img width="499" alt="스크린샷 2024-03-30 오후 10 32 49" src="https://github.com/JB243/jb243.github.io/assets/55747737/4f33b447-5f73-492f-9d64-bb01a06d6060">

<br>

⑸ **Type 5.** [χ<sup>2</sup>](https://jb243.github.io/pages/1625)

> ① For measurement data xm, ym, and the approximating function f(x),

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/e59ffc62-28d7-4ee0-89ed-ac83f3aded36)

<br>

⑹ **Type 6.**[SSIM](https://jb243.github.io/pages/2067)

> ① Image similarity comparison algorithm

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/36950528-a50f-41bf-93c2-d5067ad32c21)

<br>

> ② [Python Code](https://jb243.github.io/pages/1892)

<br>

```python
def SSIM(x, y):
    # assumption : x and y are grayscale images with the same dimension

    import numpy as np
    
    def mean(img):
        return np.mean(img)
        
    def sigma(img):
        return np.std(img)
    
    def cov(img1, img2):
        img1_ = np.array(img1[:,:], dtype=np.float64)
        img2_ = np.array(img2[:,:], dtype=np.float64)
                        
        return np.mean(img1_ * img2_) - mean(img1) * mean(img2)
    
    K1 = 0.01
    K2 = 0.03
    L = 256 # when each pixel spans 0 to 255
   
    C1 = K1 * K1 * L * L
    C2 = K2 * K2 * L * L
    C3 = C2 / 2
        
    l = (2 * mean(x) * mean(y) + C1) / (mean(x)**2 + mean(y)**2 + C1)
    c = (2 * sigma(x) * sigma(y) + C2) / (sigma(x)**2 + sigma(y)**2 + C2)
    s = (cov(x, y) + C3) / (sigma(x) * sigma(y) + C3)
        
    return l * c * s
```

<br>

⑺ **Type 7.** Mutual Information

> ① Principle **:** Can the second image be predicted given the first image?

> ② Useful for analyzing the relationship between two images obtained from different modalities

>> ○ Example **:** In MRI, T1-weighted and T2-weighted images have many inverted points; mutual information considers this.

> ③ Code

<br>

```python
def mutual_information(img1, img2):
    import numpy as np
    import cv2
    import matplotlib.pyplot as plt
    
    # img1 and img2 are 3-channel color images
    
    a = img1[:,:,0:1].reshape(img1.shape[0], img1.shape[1])
    b = img2[:,:,0:1].reshape(img2.shape[0], img2.shape[1])
    
    hgram, x_edges, y_edges = np.histogram2d(
     a.ravel(),
     b.ravel(),
     bins=20
    )

    pxy = hgram / float(np.sum(hgram))
    px = np.sum(pxy, axis=1) # marginal for x over y
    py = np.sum(pxy, axis=0) # marginal for y over x
    px_py = px[:, None] * py[None, :] # Broadcast to multiply marginals

    nzs = pxy > 0 # Only non-zero pxy values contribute to the sum
    
    return np.sum(pxy[nzs] * np.log(pxy[nzs] / px_py[nzs]))
```

<br>

> ④ [Reference](https://matthew-brett.github.io/teaching/mutual_information.html)

⑻ **Type 8.** Mr (Thresholded Mander's Colocalization Coefficient)

> ① Ratio of overlapping pixels between two different monochrome images

> ② tMr (Thresholded Mr) **:** Mr calculated considering values below a specific threshold as background with zero values

> ③ Background **:** Pearson correlation coefficient is not suitable for comparing monochrome images due to its negative values

> ④ **Feature 1.** Ranges from 0 to 1

> ⑤ **Feature 2.** Sensitive to background pixel values, but not heavily influenced by values of overlapping pixels

> ⑥ **Feature 3.** Dependent on Pearson correlation

> ⑦ **Step 1.** First, use Pearson correlation to obtain p-value and test for colocalization

> ⑧ **Step 2.** If colocalization is present, calculate tM1 and tM2 values

> ⑨ Usage **:** [ImageJ](https://www.med.unc.edu/microscopy/wp-content/uploads/sites/742/2018/06/Dr-Bob-on-Colocalization.pdf)

⑼ **Type 9.** Jaccard Similarity (IoU, intersection over union)

> ① Jaccard score **:** For two sets A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/21be186b-a0a4-4c3c-8440-68236fa0cede)

<br>

⑽ **Type 10.** Cosine Similarity

> ① Cosine value **:** For two vectors A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/037ad509-7605-4a66-a624-73cefe2e89db)

<br>

⑾ **Type 11.** Euclidean Similarity

> ① Euclidean distance **:** For two vectors A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/e8cd0340-a099-4e46-92fa-1109a9000118)

<br>

⑿ **Type 12.** Coverage Score

> ① For two sets A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/e923faa3-af63-45a8-a8ed-21279a1a1fbf)

<br>

⒀ **Type 13.** Fisher Exact Test

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/44f0ff1d-6c90-404f-8673-e31cc22a9917)

![image](https://github.com/JB243/jb243.github.io/assets/55747737/d9988700-c46e-4c8f-b61e-0f4317de8630)

<br>

> ① For two sets A and B,

⒁ **Type 14.** Faiss **:** Faiss is a library for efficient similarity search and clustering of dense vectors. Developed by Meta.

⒁ **Type 15.** Smith–Waterman similarity: Used for evaluating the similarity between nucleic acid or amino acid sequences.

<br>

---

_Input **:** 2022.08.02 16:03_

_Modified **:** 2023.08.23 14:28_
