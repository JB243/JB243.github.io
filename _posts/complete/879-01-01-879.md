## **Chapter 5-2. Types of Distances and Similarities**

Recommended Article: „ÄêStatistics„Äë Chapter 5. [Statistics](https://jb243.github.io/pages/1625)

---

**1.** [Overview](#1-overview)

**2.** [Types of Norm Concepts](#4-types-of-norm-concepts)

**3.** [Types of Distance Concepts](#3-types-of-distance-concepts)

**4.** [Types of Similarity Concepts](#4-types-of-similarity-concepts)

---

<br>

## **1\. Overview**

‚ë¥ The difference between norm and distance: often used interchangeably.

> ‚ë† norm

<br>

<img width="427" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-07 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 21 11" src="https://github.com/user-attachments/assets/a4618dd7-54e7-40ca-9c70-8d46645a2e52">

<br>

> ‚ë° distance function (metric) 

<br>

<img width="428" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-07 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 21 24" src="https://github.com/user-attachments/assets/aeba70b4-09bf-4d78-9d4b-98ead123555b">

<br>

> ‚ë¢ If a norm is defined, a distance ùëë can be defined.

<br>

<img width="164" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-07 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 21 36" src="https://github.com/user-attachments/assets/9bb9b0aa-9298-4cab-9f4a-8ddfcd7d45ac">

<br>

> ‚ë£ However, the existence of a corresponding norm is not guaranteed just because a distance is defined.

‚ëµ The difference between distance and similarity: often used interchangeably.

> ‚ë† Commonality: Saying two data points are close (short distance) is equivalent to saying they are similar. In other words, distance ‚àù 1 / similarity.

> ‚ë° Commonality: In machine learning, terms like loss function, error function, and cost function also refer to the difference between the true value and the predicted value (‚àù 1 / similarity).

> ‚ë¢ Difference: While a distance function is rigorously defined in [linear algebra](https://jb243.github.io/pages/1897), it's not necessary for loss functions or similarity measures to satisfy that definition.

‚ë∂ Various types of distances and concepts of similarity.

<br>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F7PyTd%2Fbtsy9eivA5l%2FChzu8O3KaAz31h16pNgZJk%2Fimg.png)

**Figure 1.** Various types of distances and concepts of similarity

<br>

<br>

## **2. Types of Norm Concepts**

‚ë¥ **Type 1.** L1-norm

<br>

<img width="205" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-07 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 22 04" src="https://github.com/user-attachments/assets/e765e9d5-b493-4263-acad-2a28d3a698e7">

<br>

‚ëµ **Type 2.** L2-norm

<br>

<img width="259" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-07 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 22 25" src="https://github.com/user-attachments/assets/c824996a-8724-4ea3-a6c8-3eb1d8c6c51d">

<br>

‚ë∂ **Type 3.** p-norm

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/a0154c4a-7c97-4bb0-85c1-bb8f58b54ae9)

<br>

‚ë∑ **Type 4.** Frobenius norm

<br>

<br>

## **3\. Types of Distance Concepts**

‚ë¥ **Type 1.** [L1 Loss Function](https://jb243.github.io/pages/1140) (L1-distance, MAE, city-block distance, taxicab distance, rectilinear distance, Manhattan distance)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/704693e0-2df3-4238-8aec-0af269a5466e)

<br>

> ‚ë† In other words, it is a method of calculating distance by setting paths in shapes like '„Ñ±' and '„Ñ¥'.

 ‚ëµ **Type 2.** [L2 Loss Function](https://jb243.github.io/pages/1140) (L2-distance, MSE): Euclidean distance using the Pythagorean theorem (**standard**)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/091c3d05-2507-4c39-a81a-261180769e05)

<br>

 ‚ë∂ **Type 3.** [Cross Entropy](https://jb243.github.io/pages/1140): Typically has a binary cross-entropy (BCE).

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/ad4a4661-9016-432b-bedd-6c4b2c2dbaf2)

<br>

‚ë∑ **Type 4.** Distance in [Information Theory](https://jb243.github.io/pages/2145)

<br>

![image](https://github.com/user-attachments/assets/35701e55-b2cf-4a8b-b16c-e48a919fc34d)

<br>

‚ë∏ **Type 5.** [Delaunay Triangulation](https://jb243.github.io/pages/1140)(Delaunay triangulation)

‚ëπ **Type 6.** Dot Product: Vector inner product

‚ë∫ **Type 7.** [Linkage Metric](https://jb243.github.io/pages/2150): Defines distance between clusters

‚ëª **Type 8. Hamming Distance**: Assigns a binary value to each data point and measures the distance of the data point as the difference in values.

‚ëº **Type 9. Standardized Distance**:

> ‚ë† Distance standardized by the measurement unit of the variable.

> ‚ë° Formula:

>> d(i, j)<sup>2</sup> = (X<sub>i</sub> - X<sub>j</sub>)<sup>T</sup> D<sup>-1</sup> (X<sub>i</sub> - X<sub>j</sub>) 

>> ‚óã X<sub>i</sub>: Starting point matrix

>> ‚óã X<sub>j</sub>: Endpoint matrix

>> ‚óã D: Sample variance (diagonal) matrix

‚ëΩ **Type 10.** Mahalanobis Distance**

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/d096966d-1476-463f-b019-0f85e59a97a0)

**Figure 2.** Mahalanobis Distance

<br>

> ‚ë† A statistical distance that considers both the standardization of variables and the correlation between variables (shape of the data distribution).

> ‚ë° Formula: When trying to determine the distance d between two data points X<sub>i</sub> and X<sub>j</sub>, the following formula is used:

>> d(i, j)<sup>2</sup> = (X<sub>i</sub> - X<sub>j</sub>)<sup>T</sup> S<sup>-1</sup> (X<sub>i</sub> - X<sub>j</sub>) 

>> ‚óã X<sub>i</sub> : Starting point matrix

>> ‚óã X<sub>j</sub> : Endpoint matrix

>> ‚óã S : Sample covariance matrix

> ‚ë¢ Advantages: Unlike Euclidean distance, it is scale-free, considers data correlation, and has benefits such as outlier detection.

> ‚ë£ Limitations: Assumes the normality of the data. The process of calculating the sample covariance matrix is computationally intensive.

> ‚ë§ Python code

<br>

```python
import numpy as np
from scipy.linalg import inv

def mahalanobis_distance(x, y, covariance_matrix, regularization=1e-6):
    # Add regularization to the covariance matrix's diagonal
    regularized_cov = covariance_matrix + np.eye(covariance_matrix.shape[0]) * regularization
    
    x_minus_y = np.array(x) - np.array(y)
    covariance_matrix_inv = inv(regularized_cov)
    distance = np.dot(np.dot(x_minus_y, covariance_matrix_inv), x_minus_y.T)
    return np.sqrt(distance)

# Example usage:
x = [1, 2, 3]
y = [4, 5, 6]
data = np.array([x, y])
cov_matrix = np.cov(data, rowvar=False)  # Here, we're just using the covariance of x and y for simplicity

print(mahalanobis_distance(x, y, cov_matrix))
```

<br>

‚ëæ **Type 11.** Levenshtein Distance: An algorithm that determines how similar two strings, A and B, are to each other.

‚ëø **Type 12.** Minkowski Distance

> ‚ë† Distance in m-dimensional Minkowski space.

> ‚ë° When m = 1, it is equivalent to Manhattan distance.

> ‚ë¢ When m = 2, it is equivalent to Euclidean distance.

‚íÄ **Type 13.** Hausdorff Distance

> ‚ë† Formalization: For two sets A = {a<sub>1</sub>, ..., a<sub>p</sub>} and B = {b<sub>1</sub>, ..., b<sub>q</sub>},

>> H(A, B) = max(h(A, B), h(B, A)) 

> ‚ë° directed Hausdorff distance: The distance between the two points in A and B that are furthest apart,

>> h(A, B) = max<sub>a ‚àà A</sub> min<sub>b ‚àà B</sub> <span>|</span><span>|</span> a - b <span>|</span><span>|</span>

‚íÅ **Type 14.** Focal Loss

> ‚ë† Formalization

>> FL = -(1 - P<sub>t</sub>)<sup>Œ≥</sup> log (P<sub>t</sub>)

‚íÇ **Type 15.** S√∏rensen‚ÄìDice Coefficient (Dice Distance)

> ‚ë† Formalization

>> 2 <span>|</span> A ‚à© B<span>|</span> / <span>|</span> A ‚à™ B <span>|</span>

‚íÉ **Type 16.** [Gromov-Wasserstein distance](https://jb243.github.io/pages/2386) (Kantorovich‚ÄìRubinstein metric, Earth Mover's Distance, EMD)

‚íÑ **Type 17.** Sinkhorn divergence

‚íÖ **Type 18.** Cressie-Read power divergence

‚íÜ **Type 19.** Jensen-Shannon distance

‚íá **Type 20.** total variation (TV) distance

‚íá **Type 21.** Kolmogorov-Sminrov distance

‚íá **Type 22.** Hellinger distance: Requires kernel density estimation for probability density function (pdf).

‚íá **Type 24.** Huber loss function

‚íá **Type 25.** Bhattacharyya loss 

‚íá **Type 26.** evidence lower bound (ELBO)

‚íá **Type 27.** Aitchison distance: Concept of distance in a [simplex](https://nate9389.tistory.com/2203#footnote_link_67_50).

‚íá **Type 28.** Bray Curtis distance

> BC<sub>ij</sub> = 1 - 2C<sub>ij</sub> / (S<sub>i</sub> + S<sub>j</sub>)

> ‚ë† i = one site, j = another site

> ‚ë° S<sub>i</sub> = numbers of species in i, S<sub>j</sub> = numbers of species in j

> ‚ë¢ C<sub>ij</sub> = the less number of the overlapping sites in species

‚íá **Type 29.** Fourier loss 

<br>

<br>

## **4\. Types of Similarity Concepts**

‚ë¥ **Type 1.** [Pearson Correlation Coefficient](https://jb243.github.io/pages/1625)(Pearson correlation coefficient)

> ‚ë† Given the standard deviations œÉx, œÉy of X and Y,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/9a95df8f-293f-4cf2-beb3-1eeca529cfa7)

<br>

‚ëµ **Type 2.** [Spearman Correlation Coefficient](https://jb243.github.io/pages/1625)(Spearman correlation coefficient)

> ‚ë† Define x' = rank(x) and y' = rank(x),

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/2fb73f40-0d74-41cb-bf01-31d48e691188)

<br>

‚ë∂ **Type 3.** [Kendall Correlation Coefficient](https://jb243.github.io/pages/1625#)(Kendall correlation coefficient)

> ‚ë† Define correlation for concordant and discordant pairs,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/04049609-9668-40cc-a4e2-bbe749c986cf)

<br>

‚ë∑ **Type 4.** Matthew correlation coefficient (MCC)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/5d476c4f-780b-45c4-b54a-7e937d602000)

<br>

‚ë∏ **Type 5.** [œá<sup>2</sup>](https://jb243.github.io/pages/1625)

> ‚ë† For measurement data xm, ym, and the approximating function f(x),

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/9ae571c8-3deb-4a81-8bf1-7d16835cf743)

<br>

‚ëπ **Type 6.**[SSIM](https://jb243.github.io/pages/2067)

> ‚ë† Image similarity comparison algorithm

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/b52c2038-74c0-4d36-94f7-ed244afaf72c)

<br>

> ‚ë° [Python Code](https://jb243.github.io/pages/1892)

<br>

```python
def SSIM(x, y):
    # assumption : x and y are grayscale images with the same dimension

    import numpy as np
    
    def mean(img):
        return np.mean(img)
        
    def sigma(img):
        return np.std(img)
    
    def cov(img1, img2):
        img1_ = np.array(img1[:,:], dtype=np.float64)
        img2_ = np.array(img2[:,:], dtype=np.float64)
                        
        return np.mean(img1_ * img2_) - mean(img1) * mean(img2)
    
    K1 = 0.01
    K2 = 0.03
    L = 256 # when each pixel spans 0 to 255
   
    C1 = K1 * K1 * L * L
    C2 = K2 * K2 * L * L
    C3 = C2 / 2
        
    l = (2 * mean(x) * mean(y) + C1) / (mean(x)**2 + mean(y)**2 + C1)
    c = (2 * sigma(x) * sigma(y) + C2) / (sigma(x)**2 + sigma(y)**2 + C2)
    s = (cov(x, y) + C3) / (sigma(x) * sigma(y) + C3)
        
    return l * c * s
```

<br>

‚ë∫ **Type 7.** Mutual Information

> ‚ë† Principle: Can the second image be predicted given the first image?

> ‚ë° Useful for analyzing the relationship between two images obtained from different modalities

>> ‚óã Example: In MRI, T1-weighted and T2-weighted images have many inverted points; mutual information considers this.

> ‚ë¢ Code

<br>

```python
def mutual_information(img1, img2):
    import numpy as np
    import cv2
    import matplotlib.pyplot as plt
    
    # img1 and img2 are 3-channel color images
    
    a = img1[:,:,0:1].reshape(img1.shape[0], img1.shape[1])
    b = img2[:,:,0:1].reshape(img2.shape[0], img2.shape[1])
    
    hgram, x_edges, y_edges = np.histogram2d(
     a.ravel(),
     b.ravel(),
     bins=20
    )

    pxy = hgram / float(np.sum(hgram))
    px = np.sum(pxy, axis=1) # marginal for x over y
    py = np.sum(pxy, axis=0) # marginal for y over x
    px_py = px[:, None] * py[None, :] # Broadcast to multiply marginals

    nzs = pxy > 0 # Only non-zero pxy values contribute to the sum
    
    return np.sum(pxy[nzs] * np.log(pxy[nzs] / px_py[nzs]))
```

<br>

> ‚ë£ [Reference](https://matthew-brett.github.io/teaching/mutual_information.html)

‚ëª **Type 8.** [Relative entropy](https://jb243.github.io/pages/1140)(Kullback-Leibler divergence, KL divergence, KLD)

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/aa6bf930-721a-4456-8e57-49b722663d0e)

<br>

‚ëº **Type 9.** Mr (Thresholded Mander's Colocalization Coefficient)

> ‚ë† Ratio of overlapping pixels between two different monochrome images

> ‚ë° tMr (Thresholded Mr): Mr calculated considering values below a specific threshold as background with zero values

> ‚ë¢ Background: Pearson correlation coefficient is not suitable for comparing monochrome images due to its negative values

> ‚ë£ **Feature 1.** Ranges from 0 to 1

> ‚ë§ **Feature 2.** Sensitive to background pixel values, but not heavily influenced by values of overlapping pixels

> ‚ë• **Feature 3.** Dependent on Pearson correlation

> ‚ë¶ **Step 1.** First, use Pearson correlation to obtain p-value and test for colocalization

> ‚ëß **Step 2.** If colocalization is present, calculate tM1 and tM2 values

> ‚ë® Usage: [ImageJ](https://www.med.unc.edu/microscopy/wp-content/uploads/sites/742/2018/06/Dr-Bob-on-Colocalization.pdf)

‚ëΩ **Type 10.** Jaccard Similarity (IoU, intersection over union)

> ‚ë† Jaccard score: For two sets A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/dac90ae0-752e-4aef-a071-90095950acb7)

<br>

‚ëæ **Type 11.** Cosine Similarity

> ‚ë† Cosine value: For two vectors A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/0102ad89-ca48-40ec-8ace-3aa2acbd1402)

<br>

‚ëø **Type 12.** Euclidean Similarity

> ‚ë† Euclidean distance: For two vectors A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/f07d878f-66a4-4894-a5ca-a24230700145)

<br>

‚íÄ **Type 13.** Coverage Score

> ‚ë† For two sets A and B,

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/bdbdec86-ae8b-4c58-80ee-79e7fc2cefed)

<br>

‚íÅ **Type 14.** Fisher Exact Test

<br>

![image](https://github.com/JB243/jb243.github.io/assets/55747737/74c0b388-16b7-40c1-ad41-d7edef619a18)

![image](https://github.com/JB243/jb243.github.io/assets/55747737/66cea093-5b54-4b7d-be12-8c2b46741511)

<br>

> ‚ë† For two sets A and B,

‚íÇ **Type 15.** Faiss: Faiss is a library for efficient similarity search and clustering of dense vectors. Developed by Meta.

‚íÉ **Type 16.** Smith‚ÄìWaterman similarity: Used for evaluating the similarity between nucleic acid or amino acid sequences.

‚íÑ **Type 17.** Maximal information coefficient (MIC)

‚íÖ **Type 18.** Spectral similarity: For the ùëò k-th eigenvalues ùúÜ ùê¥ ùëò Œª Ak ‚Äã and ùúÜ ùêµ ùëò Œª Bk ‚Äã of matrices ùê¥ A and ùêµ B, respectively.

<br>

<img width="268" alt="·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-10-07 ·Ñã·Ö©·Ñå·Ö•·Ü´ 11 23 57" src="https://github.com/user-attachments/assets/9468937e-8577-4ebb-b0c5-beadb54812aa">

<br>

---

_Input: 2022.08.02 16:03_

_Modified: 2023.08.23 14:28_
